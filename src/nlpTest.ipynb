{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels                                                                                                 text\n",
       "0   spam  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...\n",
       "1    ham                                        Nah I don't think he goes to usf, he lives around here though\n",
       "2    ham                        Even my brother is not like to speak with me. They treat me like aids patent.\n",
       "3    ham                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "4    ham  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "messages = pd.read_csv('../data/SMSSpamCollection.tsv', sep= '\\t')\n",
    "messages.columns = [\"labels\", \"text\"]\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  ...                                                                                           text_clean\n",
       "0   spam  ...  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...\n",
       "1    ham  ...                                          Nah I dont think he goes to usf he lives around here though\n",
       "2    ham  ...                          Even my brother is not like to speak with me They treat me like aids patent\n",
       "3    ham  ...                                                                    I HAVE A DATE ON SUNDAY WITH WILL\n",
       "4    ham  ...  As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...\n",
       "\n",
       "[5 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n      <td>As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  ...                                                                                       text_tokenized\n",
       "0   spam  ...  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...\n",
       "1    ham  ...                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]\n",
       "2    ham  ...         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]\n",
       "3    ham  ...                                                           [i, have, a, date, on, sunday, with, will]\n",
       "4    ham  ...  [as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...\n",
       "\n",
       "[5 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n      <th>text_clean</th>\n      <th>text_tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n      <td>[i, have, a, date, on, sunday, with, will]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n      <td>As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...</td>\n      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "messages['text_tokenized'] = messages['text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  ...                                                                                          text_nostop\n",
       "0   spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...\n",
       "1    ham  ...                                                 [nah, dont, think, goes, usf, lives, around, though]\n",
       "2    ham  ...                                              [even, brother, like, speak, treat, like, aids, patent]\n",
       "3    ham  ...                                                                                       [date, sunday]\n",
       "4    ham  ...  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...\n",
       "\n",
       "[5 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n      <th>text_clean</th>\n      <th>text_tokenized</th>\n      <th>text_nostop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n      <td>[i, have, a, date, on, sunday, with, will]</td>\n      <td>[date, sunday]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n      <td>As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...</td>\n      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "messages['text_nostop'] = messages['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def clean_text(text):\n",
    "     text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "     tokens = re.split('\\W+', text)\n",
    "     text = [word for word in tokens if word not in stopwords]\n",
    "     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mear', 'smell', 'smells', 'smeone', 'smidgin', 'smile', 'smiled', 'smiles', 'smiley', 'smiling', 'smith', 'smithswitch', 'smoke', 'smoked', 'smokes', 'smokin', 'smoking', 'smoothly', 'sms', 'sms08718727870', 'smsd', 'smsing', 'smsservices', 'smsshsexnetun', 'smth', 'sn', 'snake', 'snap', 'snappy', 'snatch', 'snd', 'sneham', 'snickering', 'sno', 'snogs', 'snoringthey', 'snow', 'snowball', 'snowboarding', 'snowman', 'snuggles', 'soany', 'soc', 'sochte', 'social', 'sofa', 'soft', 'software', 'soil', 'soiree', 'sol', 'soladha', 'sold', 'solihull', 'solve', 'solved', 'some1', 'somebody', 'someday', 'someone', 'someones', 'someonethat', 'someonone', 'someplace', 'somerset', 'somethin', 'something', 'somethings', 'sometime', 'sometimerakheshvisitor', 'sometimes', 'sometme', 'somewhat', 'somewhere', 'somewheresomeone', 'somewhr', 'somone', 'somtimes', 'sonathaya', 'sonetimes', 'song', 'songs', 'sonot', 'sony', 'sonyericsson', 'soo', 'soon', 'soonc', 'sooner', 'soonlots', 'soonxxx', 'sooo', 'soooo', 'sooooo', 'sophas', 'sore', 'sorrow', 'sorrowsi', 'sorry', 'sorryi', 'sorryin', 'sort', 'sorta', 'sorted', 'sortedbut', 'sorting', 'sorts', 'sory', 'sorydarealyfrm', 'sos', 'soso', 'soul', 'sound', 'sounding', 'sounds', 'soundtrack', 'soup', 'source', 'sources', 'south', 'southern', 'souveniers', 'soz', 'sp', 'space', 'spacebucks', 'spaces', 'spageddies', 'spain', 'spam', 'spanish', 'spare', 'spares', 'spark', 'sparkling', 'spatula', 'speak', 'speaking', 'special', 'specialcall', 'speciale', 'specialisation', 'specialise', 'specially', 'specific', 'specify', 'specs', 'speechless', 'speed', 'speedchat', 'speeding', 'speling', 'spell', 'spelled', 'spelling', 'spend', 'spending', 'spent', 'spice', 'spider', 'spiderman', 'spiffing', 'spile', 'spin', 'spinout', 'spiral', 'spirit', 'spiritual', 'spjanuary', 'spk', 'spl', 'splash', 'splashmobile', 'splat', 'splendid', 'split', 'splleing', 'splwat', 'spoil', 'spoiled', 'spoilt', 'spoke', 'spoken', 'sponsors', 'spontaneously', 'spook', 'spoon', 'spoons', 'sporadically', 'sport', 'sports', 'sportsx', 'spose', 'spot', 'spotty', 'spouse', 'sppok', 'spreadsheet', 'spree', 'spring', 'springs', 'sprint', 'sprwm', 'sptv', 'sptyrone', 'spunout', 'spys', 'sq825', 'squatting', 'squeeeeeze', 'squeezed', 'squid', 'squishy', 'srs', 'srsly', 'srt', 'sry', 'ssi', 'ssindia', 'ssnervous', 'st', 'stability', 'stable', 'stadium', 'staff', 'staffsciencenusedusgphyhcmkteachingpc1323', 'stage', 'stagwood', 'stairs', 'stalk', 'stalking', 'stamped', 'stamps', 'stand', 'standard', 'standing', 'stands', 'stapati', 'star', 'starer', 'staring', 'starring', 'stars', 'starshine', 'start', 'started', 'startedindia', 'starti', 'starting', 'starts', 'starve', 'starving', 'starwars3', 'stash', 'stated', 'statement', 'statements', 'station', 'stations', 'status', 'stay', 'stayed', 'stayin', 'staying', 'stays', 'std', 'stdtxtrate', 'steak', 'steal', 'stealing', 'steam', 'steamboat', 'steed', 'steering', 'step', 'steps', 'stereo', 'stereophonics', 'sterling', 'sterm', 'steve', 'stevelike', 'stewartsize', 'steyn', 'sth', 'sthis', 'stick', 'sticky', 'stifled', 'stil', 'still', 'stillmaybe', 'stink', 'stitch', 'stock', 'stocked', 'stockport', 'stolen', 'stomach', 'stomps', 'stone', 'stoners', 'stones', 'stool', 'stop', 'stop2', 'stop2stop', 'stopbcm', 'stopcost', 'stopcs', 'stopped', 'stops', 'stoptxt', 'stoptxtstop', 'store', 'storelike', 'stores', 'stories', 'storming', 'story', 'str', 'str8', 'straight', 'strain', 'strange', 'stranger', 'strangersaw', 'stream', 'street', 'streetshall', 'stress', 'stressed', 'stressful', 'stressfull', 'stretch', 'strewn', 'strict', 'strike', 'strings', 'strip', 'stripes', 'strips', 'strokes', 'strong', 'strongbuy', 'strongly', 'strt', 'strtd', 'struggling', 'sts', 'stu', 'stubborn', 'stuck', 'studdying', 'student', 'studentfinancial', 'students', 'studentsthis', 'studies', 'studio', 'study', 'studying', 'studyn', 'stuff', 'stuff42moro', 'stuffed', 'stuffing', 'stuffleaving', 'stuffs', 'stuffwhy', 'stunning', 'stupid', 'stupidits', 'style', 'styles', 'styling', 'stylish', 'stylist', 'sub', 'subject', 'subletting', 'submitted', 'submitting', 'subpoly', 'subs', 'subscribe', 'subscribe6gbpmnth', 'subscribed', 'subscriber', 'subscribers', 'subscription', 'subscriptions', 'subscriptn3gbpwk', 'subscrition', 'subsequent', 'subtoitles', 'success', 'successful', 'successfully', 'sucker', 'suckers', 'sucks', 'sudden', 'suddenly', 'sudn', 'sue', 'suffer', 'suffering', 'suffers', 'sufficient', 'sugababes', 'suganya', 'sugar', 'sugardad', 'suggest', 'suggestion', 'suggestions', 'suite', 'suitemates', 'suits', 'sullivan', 'sum', 'sum1', 'sumfing', 'summer', 'summers', 'summon', 'sumthin', 'sumthinxx', 'sun', 'sun0819', 'sunday', 'sundayish', 'sunlight', 'sunny', 'sunoco', 'sunroof', 'sunscreen', 'sunshine', 'suntec', 'sup', 'super', 'superb', 'superior', 'supervisor', 'suply', 'supose', 'suppliers', 'supplies', 'supply', 'support', 'supportproviding', 'supports', 'supportvery', 'suppose', 'supposed', 'supreme', 'suprman', 'sura', 'sure', 'surely', 'surf', 'surfing', 'surgical', 'surly', 'surname', 'surprise', 'surprised', 'surrender', 'surrounded', 'survey', 'surya', 'sutra', 'sux', 'suzy', 'svc', 'sw7', 'sw73ss', 'swalpa', 'swan', 'swann', 'swap', 'swashbuckling', 'swat', 'swatch', 'sway', 'swayze', 'swear', 'sweater', 'sweatter', 'sweet', 'sweetest', 'sweetheart', 'sweetie', 'sweets', 'swell', 'swhrt', 'swimming', 'swimsuit', 'swing', 'swiss', 'switch', 'swollen', 'swoop', 'swt', 'swtheart', 'syd', 'syllabus', 'symbol', 'sympathetic', 'symptoms', 'synced', 'syria', 'syrup', 'system', 'systems', 't91', 'ta', 'table', 'tables', 'tablet', 'tablets', 'tackle', 'tacos', 'tactful', 'tactless', 'tadaaaaa', 'tag', 'tagged', 'tahan', 'tai', 'tait', 'taj', 'taka', 'take', 'takecare', 'taken', 'takenonly', 'takes', 'takin', 'taking', 'talent', 'talents', 'talk', 'talkbut', 'talked', 'talkin', 'talking', 'talks', 'tall', 'tallahassee', 'tallent', 'tamilnaduthen', 'tampa', 'tank', 'tantrums', 'tap', 'tape', 'tariffs', 'tarot', 'tarpon', 'tas', 'taste', 'tasts', 'tat', 'tata', 'tattoos', 'tau', 'taught', 'taunton', 'taxes', 'taxi', 'taxless', 'taxt', 'taylor', 'taylors', 'tayseertissco', 'tb', 'tbspersolvo', 'tc', 'tcllc', 'tcrw1', 'tcs', 'tcsbcm4235wc1n3xx', 'tcsc', 'tcsstop', 'tddnewsletteremc1couk', 'tea', 'teach', 'teacher', 'teaches', 'teaching', 'teacoffee', 'team', 'teams', 'tear', 'tears', 'tease', 'teasing', 'tech', 'technical', 'technologies', 'tee', 'teenager', 'teeth', 'teethif', 'teethis', 'teju', 'tel', 'telephone', 'telephonic', 'teletext', 'tell', 'telling', 'tellmiss', 'tells', 'telly', 'telphone', 'telugu', 'teluguthts', 'temales', 'temp', 'temper', 'temple', 'ten', 'tenants', 'tendencies', 'tenerife', 'tensed', 'tension', 'teresa', 'term', 'terminatedwe', 'terms', 'termsapply', 'terrible', 'terrific', 'terrorist', 'terry', 'tescos', 'tessypls', 'test', 'testing', 'tests', 'tex', 'texas', 'texd', 'text', 'text82228', 'textand', 'textbook', 'textbuddy', 'textcomp', 'texted', 'textin', 'texting', 'textoperator', 'textpod', 'texts', 'textsweekend', 'tgxxrz', 'th', 'thandiyachu', 'thangam', 'thangamits', 'thank', 'thanks', 'thanks2', 'thanksgiving', 'thanku', 'thankyou', 'thanx', 'thanx4', 'thanxxx', 'thasa', 'that2worzels', 'thatd', 'thatdont', 'thati', 'thatll', 'thatmum', 'thatnow', 'thats', 'the4th', 'theacusations', 'theater', 'theatre', 'thecd', 'thedailydraw', 'thekingshead', 'themed', 'themes', 'themob', 'themobhit', 'themobyo', 'themp', 'thenwill', 'theoretically', 'theory', 'theplace', 'thepub', 'theredo', 'theregoodnight', 'therell', 'therere', 'theres', 'therexx', 'thesedays', 'theseyours', 'thesis', 'thesmszonecom', 'thewend', 'theyll', 'theyre', 'thgt', 'thia', 'thin', 'thing', 'thinghow', 'things', 'think', 'thinked', 'thinkin', 'thinking', 'thinks', 'thinkthis', 'thinl', 'thirunelvali', 'thisdon', 'thk', 'thkin', 'thm', 'thnk', 'thnq', 'thnx', 'tho', 'thoso', 'thot', 'thou', 'though', 'thought', 'thoughts', 'thoughtsi', 'thousands', 'thout', 'thread', 'threats', 'three', 'threw', 'thriller', 'throat', 'throw', 'throwin', 'throwing', 'thrown', 'throws', 'thru', 'thrurespect', 'ths', 'tht', 'thts', 'thuglyfe', 'thurs', 'thursday', 'thus', 'thx', 'thy', 'tick', 'ticket', 'tickets', 'tiempo', 'tiger', 'tight', 'tightly', 'tigress', 'tihs', 'tiime', 'til', 'till', 'tim', 'time', 'timedhoni', 'timegud', 'timehope', 'times', 'timeslil', 'timeyou', 'timeyour', 'timi', 'timin', 'timing', 'timings', 'tiny', 'tip', 'tips', 'tired', 'tiring', 'tirunelvai', 'tirunelvali', 'tirupur', 'tis', 'tisscotayseer', 'title', 'titles', 'titleso', 'tiwary', 'tix', 'tiz', 'tke', 'tkts', 'tlk', 'tm', 'tming', 'tmobile', 'tmorrowpls', 'tmr', 'tmrw', 'tmw', 'tnc', 'tncs', 'toa', 'toaday', 'tobacco', 'tobed', 'tocallshall', 'toclaim', 'today', 'todaybut', 'todaydo', 'todayfrom', 'todaygood', 'todayhe', 'todays', 'todaysundaysunday', 'todo', 'tog', 'together', 'tohar', 'toilet', 'tok', 'token', 'toking', 'tol', 'told', 'toldshe', 'toledo', 'tolerance', 'toleratbcs', 'toll', 'tom', 'tomarrow', 'tome', 'tomeandsaidthis', 'tomo', 'tomocant', 'tomorro', 'tomorrow', 'tomorrowcall', 'tomorrowtoday', 'tomorw', 'tone', 'tones', 'tones2u', 'tones2youcouk', 'tonesreply', 'tonexs', 'tonght', 'tongued', 'tonight', 'tonights', 'tonite', 'tonitebusy', 'tonitethings', 'tons', 'tonsolitusaswell', 'took', 'tookplace', 'tool', 'toolets', 'tooo', 'toopray', 'toot', 'toothpaste', 'tootsie', 'top', 'topic', 'topicsorry', 'toplay', 'topped', 'toppoly', 'tops', 'tor', 'torch', 'torrents', 'tortilla', 'torture', 'tosend', 'toshiba', 'toss', 'tot', 'total', 'totally', 'totes', 'touch', 'touched', 'tough', 'toughest', 'tour', 'towards', 'town', 'towncud', 'towndontmatter', 'toxic', 'toyota', 'tp', 'track', 'trackmarque', 'trade', 'traditions', 'traffic', 'train', 'trained', 'training', 'trainners', 'trains', 'tram', 'tranquility', 'transaction', 'transcribing', 'transfer', 'transferacc', 'transfered', 'transferred', 'transfr', 'transfred', 'transport', 'trash', 'trauma', 'trav', 'travel', 'traveling', 'travelled', 'travelling', 'treacle', 'treadmill', 'treasure', 'treat', 'treated', 'treatin', 'treats', 'trebles', 'tree', 'trek', 'trends', 'trial', 'tried', 'trip', 'triple', 'trips', 'trishul', 'triumphed', 'tron', 'trouble', 'troubleshooting', 'trouser', 'truble', 'truck', 'true', 'truekdo', 'truffles', 'truly', 'truro', 'trust', 'trusting', 'truth', 'truthful', 'try', 'tryin', 'trying', 'trywales', 'ts', 'tsandcs', 'tscs', 'tscs08714740323', 'tscs087147403231winawkage16', 'tshirt', 'tsunami', 'tsunamis', 'tt', 'tts', 'ttyl', 'tue', 'tues', 'tuesday', 'tui', 'tuition', 'tul', 'tulip', 'tunde', 'tune', 'tunji', 'turkeys', 'turn', 'turned', 'turning', 'turns', 'tuth', 'tv', 'tvhe', 'tvlol', 'twat', 'twelve', 'twenty', 'twice', 'twiggs', 'twilight', 'twinks', 'twins', 'twittering', 'two', 'tx', 'txt', 'txt250com', 'txtauction', 'txtauctiontxt', 'txtin', 'txting', 'txtjourney', 'txtno', 'txts', 'txtx', 'tyler', 'tylers', 'type', 'typelyk', 'types', 'typical', 'u', 'u2moro', 'u4', 'uawakefeellikw', 'ubandu', 'ubi', 'ucall', 'ufind', 'ugadi', 'ugh', 'ugos', 'uh', 'uhhhhrmm', 'uif', 'uin', 'ujhhhhhhh', 'uk', 'ukmobiledate', 'ukp2000', 'uks', 'ull', 'ultimate', 'ultimately', 'ultimatum', 'um', 'umma', 'ummmawill', 'ummmmmaah', 'un', 'unable', 'unbelievable', 'unclaimed', 'uncle', 'uncles', 'uncomfortable', 'unconditionally', 'unconscious', 'unconsciously', 'unconvinced', 'uncountable', 'uncut', 'underdtand', 'understand', 'understanding', 'understood', 'underwear', 'undrstnd', 'undrstndng', 'unemployed', 'uneventful', 'unfolds', 'unfortunately', 'unfortuntly', 'unhappiness', 'unhappy', 'uni', 'unicefs', 'uniform', 'unintentional', 'unintentionally', 'unique', 'uniquei', 'united', 'units', 'univ', 'university', 'unknown', 'unless', 'unlike', 'unlimited', 'unmits', 'unnecessarily', 'unni', 'unrecognized', 'unredeemed', 'unsecured', 'unsold', 'unsoldmike', 'unsoldnow', 'unspoken', 'unsub', 'unsubscribe', 'unsubscribed', 'unusual', 'uothrwise', 'up4', 'upcharge', 'upd8', 'updat', 'update', 'updatenow', 'upgrade', 'upgrading', 'upgrdcentre', 'uphad', 'upload', 'uploaded', 'uploads', 'upnot', 'upon', 'upping', 'ups', 'upset', 'upseti', 'upsetits', 'upstairs', 'upto', 'uptown', 'upyeh', 'ur', 'ure', 'urfeeling', 'urgent', 'urgentbut', 'urgentlyits', 'urgh', 'urgnt', 'urgoin', 'urgran', 'urination', 'url', 'urmomi', 'urn', 'urself', 'us', 'usb', 'usc', 'uscedu', 'use', 'used', 'useful', 'useless', 'user', 'uses', 'usf', 'usget', 'usher', 'using', 'uslet', 'usmle', 'usno', 'uso', 'usps', 'usual', 'usualiam', 'usually', 'uterus', 'utter', 'uttered', 'uu', 'uup', 'uv', 'uve', 'uwana', 'uwant', 'uworld', 'uxxxx', 'v', 'vaazhthukkal', 'vague', 'vaguely', 'vai', 'vale', 'valentine', 'valentines', 'valid', 'valid12hrs', 'valuable', 'value', 'valued', 'valuemorning', 'values', 'valuing', 'varaya', 'vargu', 'various', 'varma', 'vary', 'vasai', 'vat', 'vatian', 'vava', 'vco', 'vday', 'vegas', 'vegetables', 'veggie', 'vehicle', 'velachery', 'velly', 'velusamy', 'venaam', 'venugopal', 'verified', 'verify', 'verifying', 'version', 'versus', 'vettam', 'vewy', 'via', 'vibrant', 'vibrate', 'vibrator', 'vic', 'victoria', 'victors', 'vid', 'video', 'videochat', 'videophones', 'videopic', 'videos', 'videosound', 'videosounds2', 'vidnot', 'view', 'vijay', 'vijaykanth', 'vikky', 'vikkyim', 'vilikkamt', 'vill', 'villa', 'village', 'vinobanagar', 'violated', 'violence', 'violet', 'vip', 'virgils', 'virgin', 'virgins', 'virtual', 'visa', 'visionsmscom', 'visit', 'visiting', 'visitneed', 'visitors', 'vital', 'vitamin', 'viva', 'vivek', 'vivekanand', 'viveki', 'vl', 'vldo', 'voda', 'vodafone', 'vodka', 'voice', 'voicemail', 'voila', 'volcanoes', 'vomit', 'vomitin', 'vomiting', 'vote', 'voted', 'voucher', 'vouchers', 'voucherstext', 'vpist', 'vpod', 'vry', 'vs', 'vth', 'vtired', 'vu', 'w', 'w111wx', 'w14rg', 'w1a', 'w1j', 'w1t1jy', 'w4', 'w45wq', 'w8in', 'wa', 'wa14', 'waaaat', 'wad', 'wadebridgei', 'wah', 'wahala', 'wahay', 'waheed', 'waheeda', 'wahleykkumsharing', 'waht', 'wait', 'waited', 'waiti', 'waitin', 'waiting', 'waitshould', 'waitu', 'wake', 'waking', 'wales', 'waliking', 'walk', 'walkabout', 'walked', 'walkin', 'walking', 'walks', 'wall', 'wallet', 'wallpaper', 'wallpaperall', 'walls', 'walmart', 'walsall', 'wamma', 'wan', 'wan2', 'wana', 'wanna', 'wannatell', 'want', 'want2come', 'wanted', 'wanting', 'wants', 'wap', 'waqt', 'warm', 'warming', 'warned', 'warner', 'warning', 'warranty', 'warwick', 'washob', 'wasnt', 'waste', 'wasted', 'wasting', 'wat', 'watch', 'watched', 'watches', 'watchin', 'watching', 'watchng', 'water', 'watever', 'watevr', 'wating', 'watll', 'watrdayno', 'wats', 'watts', 'waves', 'way', 'way2smscom', 'waythis', 'wc', 'wc1n', 'wc1n3xx', 'weak', 'weakness', 'weaknesses', 'weapon', 'wear', 'wearing', 'weaseling', 'weasels', 'weather', 'weathers', 'web', 'web2mobile', 'webadres', 'webeburnin', 'webpage', 'website', 'websitenow', 'wed', 'weddin', 'wedding', 'weddingfriend', 'wedlunch', 'wednesday', 'weds', 'wee', 'weed', 'weeddeficient', 'week', 'weekdays', 'weekend', 'weekends', 'weekly', 'weeks', 'weekstop', 'weigh', 'weighed', 'weight', 'weighthaha', 'weightloss', 'weird', 'weirdest', 'weirdo', 'weirdy', 'weiyi', 'welcome', 'welcomes', 'well', 'wellda', 'welli', 'welltake', 'wellyou', 'welp', 'wen', 'wendy', 'wenever', 'went', 'wenwecan', 'wer', 'wereare', 'werebored', 'werent', 'werethe', 'wesley', 'wesleys', 'west', 'western', 'westlife', 'westonzoyland', 'westshore', 'wet', 'wetherspoons', 'weve', 'wewa', 'whassup', 'whatever', 'whats', 'whatsup', 'wheat', 'wheel', 'wheellock', 'whenever', 'whenevr', 'whenre', 'whens', 'whenwhere', 'whereare', 'wherebtw', 'wheres', 'wherever', 'wherevr', 'wherres', 'whether', 'whileamp', 'whilltake', 'whispers', 'white', 'whn', 'whole', 'whore', 'whos', 'whose', 'whr', 'wi', 'wicked', 'wicket', 'wicklow', 'wid', 'widelivecomindex', 'wif', 'wife', 'wifedont', 'wifehow', 'wifes', 'wifi', 'wihtuot', 'wikipediacom', 'wil', 'wild', 'wildest', 'wildlife', 'willing', 'willpower', 'win', 'win150ppmx3age16', 'wind', 'window', 'windows', 'winds', 'windy', 'wine', 'wined', 'wings', 'wining', 'winner', 'winnersclub', 'winning', 'wins', 'winterstone', 'wipe', 'wipro', 'wiproyou', 'wire3net', 'wisdom', 'wise', 'wish', 'wisheds', 'wishes', 'wishin', 'wishing', 'wishlist', 'wiskey', 'wit', 'withdraw', 'wither', 'within', 'without', 'witin', 'witot', 'witout', 'wiv', 'wizzle', 'wk', 'wkend', 'wkent150p16', 'wkg', 'wkly', 'wknd', 'wks', 'wktxt', 'wlcome', 'wld', 'wmlid1b6a5ecef91ff937819firsttrue180430jul05', 'wmlid820554ad0a1705572711firsttrue', 'wn', 'wnevr', 'wnt', 'wo', 'woah', 'wocay', 'woke', 'woken', 'woman', 'womdarfull', 'women', 'wondar', 'wondarfull', 'wonder', 'wonderful', 'wondering', 'wonders', 'wont', 'woo', 'woodland', 'woods', 'woohoo', 'woot', 'woould', 'woozles', 'worc', 'word', 'wordcollect', 'wordnot', 'words', 'wordsevry', 'wordstart', 'work', 'workage', 'workand', 'workin', 'working', 'worklove', 'workout', 'works', 'world', 'worldgnun', 'worldmay', 'worlds', 'worldvery', 'worms', 'worried', 'worriedx', 'worries', 'worry', 'worryc', 'worrying', 'worryuse', 'worse', 'worst', 'worth', 'worthless', 'wot', 'wotu', 'wotz', 'woul', 'would', 'woulda', 'wouldnt', 'wounds', 'wow', 'wquestion', 'wrc', 'wrecked', 'wrench', 'wrenching', 'wright', 'write', 'writhing', 'wrk', 'wrki', 'wrkin', 'wrking', 'wrks', 'wrld', 'wrnog', 'wrong', 'wrongly', 'wrongtake', 'wrote', 'ws', 'wt', 'wtc', 'wtf', 'wth', 'wthout', 'wud', 'wudnt', 'wuld', 'wuldnt', 'wun', 'www07781482378com', 'www4tcbiz', 'www80488biz', 'wwwapplausestorecom', 'wwwareyouuniquecouk', 'wwwasjesuscom', 'wwwb4utelecom', 'wwwbridalpetticoatdreamscouk', 'wwwcashbincouk', 'wwwclubmobycom', 'wwwclubzedcouk', 'wwwcnupdatescomnewsletter', 'wwwcomuknet', 'wwwdbuknet', 'wwwflirtpartyus', 'wwwfullonsmscom', 'wwwgambtv', 'wwwgetzedcouk', 'wwwidewcom', 'wwwldewcom', 'wwwldewcom1win150ppmx3age16', 'wwwldewcom1win150ppmx3age16subscription', 'wwwldewcomsubs161win150ppmx3', 'wwwmovietriviatv', 'wwwmusictrivianet', 'wwworangecoukow', 'wwwphb1com', 'wwwregalportfoliocouk', 'wwwringtonekingcouk', 'wwwringtonescouk', 'wwwrtfsphostingcom', 'wwwsantacallingcom', 'wwwshortbreaksorguk', 'wwwsmsacubootydelious', 'wwwsmsacugoldviking', 'wwwsmsacuhmmross', 'wwwsmsacunat27081980', 'wwwsmsacunatalie2k9', 'wwwsmsconet', 'wwwtcbiz', 'wwwtelediscountcouk', 'wwwtextcompcom', 'wwwtextpodnet', 'wwwtklscom', 'wwwtxt2shopcom', 'wwwtxt43com', 'wwwtxt82228com', 'wwwtxttowincouk', 'wwwwin82050couk', 'wylie', 'x', 'x2', 'x29', 'x49', 'x49your', 'xafter', 'xam', 'xavier', 'xchat', 'xclusiveclubsaisai', 'xin', 'xins', 'xmas', 'xnet', 'xoxo', 'xt', 'xuhui', 'xx', 'xxsp', 'xxuk', 'xxx', 'xxxmobilemovieclub', 'xxxmobilemovieclubcomnqjkgighjjgcbl', 'xxxx', 'xxxxx', 'xxxxxx', 'xxxxxxx', 'xxxxxxxx', 'xxxxxxxxxxxxxx', 'xy', 'y87', 'ya', 'yagoing', 'yah', 'yahoo', 'yalrigu', 'yalru', 'yam', 'yan', 'yar', 'yards', 'yavnt', 'yaxx', 'yaxxx', 'yay', 'yck', 'yday', 'yeah', 'yeahand', 'year', 'years', 'yeesh', 'yeh', 'yelling', 'yellow', 'yelowi', 'yen', 'yeovil', 'yep', 'yer', 'yes', 'yes165', 'yes434', 'yes440', 'yes762', 'yes910', 'yesbut', 'yesfrom', 'yesgauti', 'yeshe', 'yeshere', 'yesim', 'yesmum', 'yessura', 'yest', 'yesterday', 'yet', 'yettys', 'yetunde', 'yi', 'yifeng', 'yijue', 'yijuehotmailcom', 'ym', 'ymca', 'yo', 'yoga', 'yogasana', 'yohere', 'yor', 'yorge', 'youany', 'youcarlos', 'youclean', 'youd', 'youdearwith', 'youdoing', 'youhow', 'youi', 'youkwhere', 'yould', 'youll', 'youmoney', 'youmy', 'young', 'younger', 'youphone', 'youre', 'yourinclusive', 'yourjob', 'youso', 'youthats', 'youto', 'youuuuu', 'youve', 'youwanna', 'youwhen', 'yoville', 'yowifes', 'yoyyooo', 'yr', 'yrs', 'ystrdayice', 'yummmm', 'yummy', 'yun', 'yunny', 'yuo', 'yuou', 'yup', 'yupz', 'ywhere', 'z', 'zac', 'zahers', 'zealand', 'zebra', 'zed', 'zeros', 'zhong', 'zindgi', 'zoe', 'zogtorius', 'zoom', 'zouk', 'zyada', 'é', 'ü', 'üll', '〨ud']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfid_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "X_tfidf = tfid_vect.fit_transform(messages['text'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfid_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = pd.DataFrame(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, messages['labels'], test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 1.0 / Recall: 0.81\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "print('Precision: {} / Recall: {}'.format(round(precision, 3), round(recall, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 3.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.1.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 10.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/mishkasistrunk/.local/lib/python3.8/site-packages (from gensim) (1.19.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-4.1.0-py3-none-any.whl size=106204 sha256=3107ff4318ab09bf6c498e86632f14fb3ed2203a3b8a4c4ba34c416e81ad6d18\n",
      "  Stored in directory: /home/mishkasistrunk/.cache/pip/wheels/74/33/8e/37e22f50ce94856f37b3e23a93c648c194aa8d5a546083d09a\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-4.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/mishkasistrunk/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "wiki_embeddings = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "wiki_embeddings['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('prince', 0.7682329416275024),\n",
       " ('queen', 0.7507690191268921),\n",
       " ('son', 0.7020887136459351),\n",
       " ('brother', 0.6985775232315063),\n",
       " ('monarch', 0.6977890729904175),\n",
       " ('throne', 0.6919990181922913),\n",
       " ('kingdom', 0.6811410188674927),\n",
       " ('father', 0.6802029013633728),\n",
       " ('emperor', 0.6712857484817505),\n",
       " ('ii', 0.6676074266433716)]"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "wiki_embeddings.most_similar('king')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  ...                                                                                          text_nostop\n",
       "0   spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...\n",
       "1    ham  ...                                                 [nah, dont, think, goes, usf, lives, around, though]\n",
       "2    ham  ...                                              [even, brother, like, speak, treat, like, aids, patent]\n",
       "3    ham  ...                                                                                       [date, sunday]\n",
       "4    ham  ...  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...\n",
       "\n",
       "[5 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n      <th>text_clean</th>\n      <th>text_tokenized</th>\n      <th>text_nostop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>[have, date, on, sunday, with, will]</td>\n      <td>[i, have, a, date, on, sunday, with, will]</td>\n      <td>[date, sunday]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'], messages['labels'], test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                size = 100,\n",
    "                                window = 5,\n",
    "                                min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.0027289 , -0.01344087, -0.00340273,  0.01170433, -0.09169315,\n",
       "        0.03231518,  0.04601553, -0.01869143,  0.00514376,  0.01020341,\n",
       "       -0.05863426,  0.01493125,  0.03770757,  0.0087204 , -0.0922171 ,\n",
       "       -0.02992693, -0.03780801, -0.08308336, -0.07739293, -0.02217375,\n",
       "       -0.0727206 , -0.00615059, -0.00844011, -0.00017599,  0.02643425,\n",
       "        0.03070591,  0.00876151, -0.0268494 ,  0.03228621,  0.04342809,\n",
       "       -0.01244085,  0.03023546, -0.00804392,  0.02696636,  0.03505045,\n",
       "        0.02275912, -0.0086491 , -0.07379606,  0.02198456,  0.01213531,\n",
       "        0.03617238,  0.00607156,  0.03259821,  0.03730531, -0.04976666,\n",
       "        0.0140271 ,  0.02522767,  0.02650619, -0.04305142,  0.01218043,\n",
       "        0.01614249,  0.02814395,  0.00253435,  0.03243604,  0.01847801,\n",
       "       -0.04655145,  0.0021002 ,  0.03138082,  0.02909566, -0.0325501 ,\n",
       "       -0.03871831, -0.04268863, -0.04270626, -0.00037604, -0.08208974,\n",
       "        0.04902735, -0.05325377, -0.00378029, -0.05680966,  0.01348435,\n",
       "       -0.0166907 , -0.01823265,  0.00683338, -0.01718999, -0.00641952,\n",
       "       -0.07125638,  0.01564479,  0.00034191, -0.01313375,  0.02443852,\n",
       "        0.00057249,  0.01298177,  0.02049926, -0.0368581 ,  0.00100412,\n",
       "       -0.06482978,  0.07654961, -0.06007746, -0.06781342, -0.0338524 ,\n",
       "        0.04050899, -0.02835063,  0.02806822,  0.00102409, -0.02442429,\n",
       "        0.02214144,  0.00205328,  0.06389648, -0.04550013,  0.00126951],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "w2v_model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('co', 0.9977072477340698),\n",
       " ('sweet', 0.9976955652236938),\n",
       " ('once', 0.9976819753646851),\n",
       " ('love', 0.9976751208305359),\n",
       " ('haha', 0.9976654052734375),\n",
       " ('tot', 0.9976487159729004),\n",
       " ('thanks', 0.9976344704627991),\n",
       " ('long', 0.9976239204406738),\n",
       " ('half', 0.997617244720459),\n",
       " ('heart', 0.9976092576980591)]"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in w2v_model.wv.index2word]) for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20 20\n13 13\n15 15\n5 3\n16 14\n24 21\n13 12\n12 11\n5 4\n7 6\n6 5\n1 1\n25 22\n8 7\n19 19\n8 7\n30 30\n6 6\n21 21\n1 1\n4 4\n22 21\n7 6\n37 34\n19 19\n9 8\n25 25\n26 20\n10 10\n24 21\n2 2\n10 7\n2 2\n23 16\n7 6\n16 15\n27 25\n31 29\n6 6\n15 12\n7 5\n27 6\n4 4\n11 11\n11 9\n12 10\n10 8\n6 6\n6 5\n21 19\n18 16\n15 15\n18 18\n12 11\n12 10\n19 12\n24 18\n30 26\n10 8\n7 6\n18 16\n23 21\n0 0\n6 6\n24 20\n25 22\n25 24\n24 20\n4 4\n7 5\n6 6\n27 27\n31 29\n29 25\n32 32\n16 16\n17 17\n11 9\n15 14\n8 8\n25 20\n43 40\n11 11\n6 6\n20 19\n1 1\n9 8\n19 18\n6 6\n5 5\n21 19\n5 5\n11 10\n30 25\n5 5\n11 11\n9 8\n19 18\n3 3\n8 8\n6 6\n5 5\n12 12\n6 6\n7 7\n12 9\n32 28\n5 4\n5 5\n19 17\n30 26\n11 7\n24 20\n11 10\n2 0\n14 13\n13 11\n12 12\n11 11\n27 23\n16 14\n8 7\n25 22\n28 26\n9 9\n10 9\n24 24\n3 1\n26 24\n12 12\n12 11\n14 14\n8 8\n9 9\n3 3\n4 4\n44 41\n8 7\n13 11\n20 18\n9 5\n22 21\n52 46\n13 13\n17 15\n5 4\n21 20\n17 14\n31 26\n4 4\n23 23\n12 12\n17 16\n17 17\n5 5\n27 24\n26 22\n5 5\n79 75\n7 7\n20 17\n4 4\n26 26\n9 9\n16 13\n5 5\n27 24\n8 7\n12 12\n7 6\n22 22\n23 23\n11 10\n25 23\n29 26\n7 7\n15 14\n14 12\n26 19\n6 6\n5 4\n19 16\n4 4\n28 18\n9 9\n20 18\n47 40\n20 19\n4 4\n18 16\n26 25\n19 17\n7 6\n10 10\n9 8\n26 23\n18 18\n9 9\n12 11\n8 8\n21 21\n18 17\n30 30\n9 7\n14 13\n40 33\n11 10\n12 9\n8 7\n21 19\n6 5\n8 7\n26 25\n3 3\n5 4\n6 4\n15 14\n26 26\n4 4\n8 8\n31 27\n4 4\n49 39\n26 24\n13 11\n9 9\n10 9\n10 8\n21 13\n17 14\n6 6\n13 13\n12 12\n9 8\n19 19\n5 5\n10 9\n10 10\n14 14\n6 6\n6 6\n11 10\n4 4\n5 3\n20 17\n5 5\n27 27\n7 7\n26 24\n5 5\n5 3\n16 15\n21 21\n12 12\n5 4\n11 9\n19 15\n22 21\n10 9\n25 24\n26 24\n17 16\n13 13\n24 24\n6 6\n19 16\n24 20\n21 20\n5 5\n17 17\n14 13\n30 25\n29 29\n26 26\n14 13\n5 5\n5 5\n22 17\n10 10\n30 25\n21 16\n4 4\n11 10\n21 19\n25 25\n12 11\n12 10\n19 15\n6 6\n7 7\n10 10\n8 8\n5 4\n9 9\n68 50\n17 15\n31 30\n13 10\n29 29\n6 6\n18 16\n9 9\n14 12\n5 4\n9 9\n10 10\n21 17\n4 4\n26 26\n8 7\n30 28\n17 15\n21 21\n11 10\n6 6\n22 21\n5 5\n8 7\n15 15\n5 5\n6 4\n12 12\n2 2\n9 8\n5 4\n9 9\n8 8\n5 4\n13 13\n22 21\n16 14\n13 11\n4 4\n1 1\n8 7\n2 2\n23 23\n26 26\n33 33\n13 13\n8 7\n25 24\n21 19\n60 60\n13 13\n8 6\n4 4\n7 6\n14 13\n11 11\n15 14\n10 10\n6 5\n26 22\n16 14\n13 10\n23 23\n6 6\n13 13\n6 6\n7 5\n6 4\n33 25\n21 20\n22 22\n1 1\n11 6\n24 24\n6 6\n4 4\n14 14\n7 7\n25 19\n26 24\n9 9\n7 6\n16 16\n13 12\n23 22\n22 21\n23 23\n46 46\n22 11\n4 4\n12 10\n14 12\n12 10\n16 15\n8 8\n4 2\n24 23\n13 7\n20 20\n16 13\n5 5\n4 4\n9 8\n31 29\n7 6\n28 28\n7 6\n34 33\n4 4\n9 7\n19 18\n5 5\n21 19\n5 5\n16 14\n8 8\n26 26\n4 4\n6 6\n10 8\n4 3\n4 2\n5 5\n29 29\n24 21\n9 9\n4 4\n6 6\n7 7\n21 21\n37 32\n6 6\n14 14\n16 11\n15 14\n18 16\n6 6\n18 16\n23 21\n10 10\n10 9\n32 29\n22 22\n4 4\n28 27\n5 5\n5 4\n4 4\n8 8\n0 0\n7 7\n16 16\n17 17\n6 6\n7 7\n6 5\n8 5\n21 21\n7 6\n12 11\n34 32\n11 10\n16 15\n24 16\n4 4\n5 5\n11 11\n8 8\n19 19\n7 6\n9 9\n22 20\n27 26\n23 22\n9 8\n22 21\n16 13\n8 8\n19 17\n10 10\n14 13\n18 16\n5 5\n26 22\n6 5\n24 24\n7 4\n18 17\n6 5\n4 3\n29 27\n28 23\n13 10\n10 7\n24 23\n22 21\n7 7\n10 10\n5 4\n5 4\n15 15\n11 11\n13 12\n12 7\n36 22\n9 9\n24 21\n7 7\n5 5\n6 6\n5 4\n4 3\n11 11\n8 7\n14 12\n17 16\n6 5\n8 6\n18 15\n21 16\n24 23\n7 7\n3 3\n25 24\n10 8\n31 30\n22 21\n21 20\n30 25\n12 10\n22 22\n7 7\n8 8\n4 4\n20 17\n18 16\n14 12\n8 8\n20 17\n16 14\n7 7\n23 22\n24 22\n36 33\n6 4\n7 7\n12 9\n9 7\n4 4\n6 5\n8 6\n10 8\n22 20\n25 24\n24 24\n21 18\n5 5\n5 3\n25 25\n7 7\n7 7\n9 9\n5 5\n28 25\n6 5\n2 2\n14 14\n8 7\n15 15\n8 8\n30 30\n5 2\n6 5\n25 25\n12 12\n8 8\n19 17\n17 17\n14 14\n14 14\n27 24\n6 6\n9 9\n6 6\n22 20\n22 22\n29 26\n9 9\n3 3\n8 7\n9 8\n16 15\n18 15\n6 6\n5 5\n16 14\n28 28\n18 11\n26 22\n9 9\n19 15\n9 9\n6 6\n5 4\n25 22\n5 3\n5 5\n3 3\n16 16\n9 8\n15 15\n6 6\n8 8\n15 15\n21 18\n11 11\n12 9\n27 25\n11 10\n30 22\n6 5\n17 14\n30 30\n4 4\n2 2\n16 14\n23 22\n25 24\n21 21\n8 8\n26 25\n10 10\n10 9\n10 9\n5 5\n6 4\n13 12\n13 8\n38 31\n5 5\n12 12\n9 7\n21 21\n5 4\n20 20\n9 9\n13 7\n9 7\n5 5\n23 22\n8 7\n11 7\n4 4\n25 25\n5 5\n24 22\n6 6\n16 16\n22 22\n17 13\n5 5\n23 22\n21 20\n16 12\n12 12\n12 12\n9 9\n20 20\n24 19\n31 26\n20 20\n25 25\n12 12\n7 6\n11 11\n5 4\n27 25\n27 27\n28 27\n13 12\n14 14\n17 9\n4 4\n21 15\n26 22\n4 4\n16 10\n14 13\n9 8\n22 17\n13 8\n3 3\n16 14\n17 17\n8 8\n10 9\n7 6\n23 22\n5 3\n21 20\n12 10\n8 3\n11 10\n27 26\n6 6\n10 10\n4 4\n42 39\n8 8\n10 9\n7 7\n14 14\n15 15\n6 5\n14 14\n5 4\n10 9\n8 8\n9 5\n13 12\n11 11\n25 25\n14 13\n7 6\n31 31\n4 4\n22 19\n5 4\n25 25\n19 18\n22 19\n5 5\n23 22\n9 6\n15 14\n9 9\n6 6\n6 6\n36 33\n18 16\n27 20\n7 7\n14 14\n21 21\n21 21\n3 1\n23 20\n13 10\n26 20\n6 3\n10 10\n6 5\n6 6\n24 21\n13 12\n3 2\n27 27\n7 6\n4 3\n28 25\n14 12\n25 21\n15 8\n30 29\n8 8\n24 20\n9 4\n34 33\n12 11\n25 22\n24 19\n6 6\n9 9\n8 2\n29 26\n17 16\n5 4\n11 10\n5 5\n4 3\n16 12\n6 6\n11 8\n27 26\n8 6\n30 26\n4 4\n7 7\n8 8\n11 9\n13 12\n4 3\n9 8\n23 21\n17 12\n19 13\n10 10\n7 7\n28 26\n31 31\n5 1\n4 4\n4 4\n9 8\n22 19\n17 17\n5 5\n13 13\n13 12\n14 12\n6 6\n4 4\n6 6\n8 7\n23 23\n24 23\n20 17\n5 4\n29 28\n4 4\n6 6\n20 17\n4 3\n11 11\n6 6\n17 14\n21 20\n19 19\n13 12\n4 4\n6 6\n5 4\n18 15\n8 7\n9 8\n23 21\n32 29\n12 12\n8 8\n12 9\n18 18\n18 15\n31 31\n5 2\n10 8\n38 34\n4 2\n24 20\n23 16\n14 14\n19 12\n5 5\n14 14\n4 3\n25 21\n9 9\n30 30\n23 17\n11 10\n8 8\n7 5\n21 21\n23 22\n4 3\n3 1\n16 15\n26 18\n12 7\n27 25\n9 6\n7 7\n4 2\n2 2\n14 10\n26 23\n16 15\n17 16\n7 5\n2 2\n58 52\n11 11\n17 15\n4 4\n10 9\n12 12\n11 9\n19 17\n12 9\n7 7\n9 8\n23 22\n6 6\n60 53\n7 6\n11 11\n4 4\n8 7\n2 2\n13 12\n14 14\n14 13\n8 7\n22 21\n5 4\n29 28\n19 17\n8 8\n7 7\n6 6\n21 21\n5 5\n6 5\n24 23\n9 9\n9 9\n21 20\n14 14\n17 17\n6 5\n4 3\n13 12\n5 5\n12 10\n5 4\n30 28\n20 20\n7 6\n26 25\n6 6\n11 11\n6 6\n9 8\n9 7\n9 9\n6 5\n12 12\n9 8\n21 20\n25 23\n16 15\n27 8\n14 13\n14 12\n30 25\n10 10\n1 0\n5 4\n6 6\n22 18\n51 44\n6 6\n13 11\n5 5\n6 6\n12 12\n8 8\n3 2\n4 4\n18 18\n13 13\n31 29\n19 19\n8 8\n4 4\n30 28\n13 12\n1 1\n1 1\n5 5\n8 8\n11 11\n14 14\n4 3\n12 11\n9 9\n13 11\n16 16\n6 6\n7 6\n10 8\n5 5\n13 12\n11 11\n11 11\n7 5\n6 6\n6 5\n29 28\n18 16\n5 5\n22 21\n19 19\n13 11\n28 27\n5 4\n11 10\n6 6\n20 18\n33 31\n12 12\n26 26\n5 5\n9 9\n23 15\n19 18\n23 23\n7 5\n23 23\n26 26\n17 16\n9 9\n5 4\n6 6\n16 13\n9 6\n8 4\n5 5\n6 6\n28 25\n8 7\n5 5\n14 14\n4 4\n9 9\n16 15\n12 11\n5 5\n8 7\n22 21\n3 1\n10 10\n12 10\n2 2\n46 40\n5 5\n6 6\n6 6\n25 22\n11 8\n13 12\n23 23\n5 4\n10 10\n4 4\n12 11\n15 15\n22 22\n8 6\n28 28\n9 6\n1 1\n21 19\n29 28\n14 13\n8 7\n9 9\n8 6\n29 29\n4 4\n5 5\n7 5\n8 8\n6 6\n11 11\n10 8\n30 29\n12 12\n26 26\n23 23\n5 5\n27 26\n11 8\n10 10\n4 2\n9 9\n11 10\n5 4\n13 11\n18 18\n19 17\n7 5\n7 4\n7 7\n5 5\n6 6\n6 6\n11 10\n15 11\n9 7\n8 7\n7 5\n6 6\n9 8\n5 5\n8 7\n11 11\n10 10\n14 11\n5 5\n18 17\n24 20\n22 18\n26 25\n11 10\n30 28\n7 7\n22 16\n26 24\n25 25\n9 8\n13 11\n11 11\n8 7\n12 11\n6 6\n8 8\n11 11\n16 15\n15 12\n5 3\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(w2v_vect):\n",
    "    print(len(X_test.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vect_avg = []\n",
    "\n",
    "#returns word vector averages across each post\n",
    "for vect in w2v_vect:\n",
    "    if len(vect)!= 0:\n",
    "        w2v_vect_avg.append(vect.mean(axis=0))\n",
    "    else:\n",
    "            w2v_vect_avg.append(np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20 100\n13 100\n15 100\n5 100\n16 100\n24 100\n13 100\n12 100\n5 100\n7 100\n6 100\n1 100\n25 100\n8 100\n19 100\n8 100\n30 100\n6 100\n21 100\n1 100\n4 100\n22 100\n7 100\n37 100\n19 100\n9 100\n25 100\n26 100\n10 100\n24 100\n2 100\n10 100\n2 100\n23 100\n7 100\n16 100\n27 100\n31 100\n6 100\n15 100\n7 100\n27 100\n4 100\n11 100\n11 100\n12 100\n10 100\n6 100\n6 100\n21 100\n18 100\n15 100\n18 100\n12 100\n12 100\n19 100\n24 100\n30 100\n10 100\n7 100\n18 100\n23 100\n0 100\n6 100\n24 100\n25 100\n25 100\n24 100\n4 100\n7 100\n6 100\n27 100\n31 100\n29 100\n32 100\n16 100\n17 100\n11 100\n15 100\n8 100\n25 100\n43 100\n11 100\n6 100\n20 100\n1 100\n9 100\n19 100\n6 100\n5 100\n21 100\n5 100\n11 100\n30 100\n5 100\n11 100\n9 100\n19 100\n3 100\n8 100\n6 100\n5 100\n12 100\n6 100\n7 100\n12 100\n32 100\n5 100\n5 100\n19 100\n30 100\n11 100\n24 100\n11 100\n2 100\n14 100\n13 100\n12 100\n11 100\n27 100\n16 100\n8 100\n25 100\n28 100\n9 100\n10 100\n24 100\n3 100\n26 100\n12 100\n12 100\n14 100\n8 100\n9 100\n3 100\n4 100\n44 100\n8 100\n13 100\n20 100\n9 100\n22 100\n52 100\n13 100\n17 100\n5 100\n21 100\n17 100\n31 100\n4 100\n23 100\n12 100\n17 100\n17 100\n5 100\n27 100\n26 100\n5 100\n79 100\n7 100\n20 100\n4 100\n26 100\n9 100\n16 100\n5 100\n27 100\n8 100\n12 100\n7 100\n22 100\n23 100\n11 100\n25 100\n29 100\n7 100\n15 100\n14 100\n26 100\n6 100\n5 100\n19 100\n4 100\n28 100\n9 100\n20 100\n47 100\n20 100\n4 100\n18 100\n26 100\n19 100\n7 100\n10 100\n9 100\n26 100\n18 100\n9 100\n12 100\n8 100\n21 100\n18 100\n30 100\n9 100\n14 100\n40 100\n11 100\n12 100\n8 100\n21 100\n6 100\n8 100\n26 100\n3 100\n5 100\n6 100\n15 100\n26 100\n4 100\n8 100\n31 100\n4 100\n49 100\n26 100\n13 100\n9 100\n10 100\n10 100\n21 100\n17 100\n6 100\n13 100\n12 100\n9 100\n19 100\n5 100\n10 100\n10 100\n14 100\n6 100\n6 100\n11 100\n4 100\n5 100\n20 100\n5 100\n27 100\n7 100\n26 100\n5 100\n5 100\n16 100\n21 100\n12 100\n5 100\n11 100\n19 100\n22 100\n10 100\n25 100\n26 100\n17 100\n13 100\n24 100\n6 100\n19 100\n24 100\n21 100\n5 100\n17 100\n14 100\n30 100\n29 100\n26 100\n14 100\n5 100\n5 100\n22 100\n10 100\n30 100\n21 100\n4 100\n11 100\n21 100\n25 100\n12 100\n12 100\n19 100\n6 100\n7 100\n10 100\n8 100\n5 100\n9 100\n68 100\n17 100\n31 100\n13 100\n29 100\n6 100\n18 100\n9 100\n14 100\n5 100\n9 100\n10 100\n21 100\n4 100\n26 100\n8 100\n30 100\n17 100\n21 100\n11 100\n6 100\n22 100\n5 100\n8 100\n15 100\n5 100\n6 100\n12 100\n2 100\n9 100\n5 100\n9 100\n8 100\n5 100\n13 100\n22 100\n16 100\n13 100\n4 100\n1 100\n8 100\n2 100\n23 100\n26 100\n33 100\n13 100\n8 100\n25 100\n21 100\n60 100\n13 100\n8 100\n4 100\n7 100\n14 100\n11 100\n15 100\n10 100\n6 100\n26 100\n16 100\n13 100\n23 100\n6 100\n13 100\n6 100\n7 100\n6 100\n33 100\n21 100\n22 100\n1 100\n11 100\n24 100\n6 100\n4 100\n14 100\n7 100\n25 100\n26 100\n9 100\n7 100\n16 100\n13 100\n23 100\n22 100\n23 100\n46 100\n22 100\n4 100\n12 100\n14 100\n12 100\n16 100\n8 100\n4 100\n24 100\n13 100\n20 100\n16 100\n5 100\n4 100\n9 100\n31 100\n7 100\n28 100\n7 100\n34 100\n4 100\n9 100\n19 100\n5 100\n21 100\n5 100\n16 100\n8 100\n26 100\n4 100\n6 100\n10 100\n4 100\n4 100\n5 100\n29 100\n24 100\n9 100\n4 100\n6 100\n7 100\n21 100\n37 100\n6 100\n14 100\n16 100\n15 100\n18 100\n6 100\n18 100\n23 100\n10 100\n10 100\n32 100\n22 100\n4 100\n28 100\n5 100\n5 100\n4 100\n8 100\n0 100\n7 100\n16 100\n17 100\n6 100\n7 100\n6 100\n8 100\n21 100\n7 100\n12 100\n34 100\n11 100\n16 100\n24 100\n4 100\n5 100\n11 100\n8 100\n19 100\n7 100\n9 100\n22 100\n27 100\n23 100\n9 100\n22 100\n16 100\n8 100\n19 100\n10 100\n14 100\n18 100\n5 100\n26 100\n6 100\n24 100\n7 100\n18 100\n6 100\n4 100\n29 100\n28 100\n13 100\n10 100\n24 100\n22 100\n7 100\n10 100\n5 100\n5 100\n15 100\n11 100\n13 100\n12 100\n36 100\n9 100\n24 100\n7 100\n5 100\n6 100\n5 100\n4 100\n11 100\n8 100\n14 100\n17 100\n6 100\n8 100\n18 100\n21 100\n24 100\n7 100\n3 100\n25 100\n10 100\n31 100\n22 100\n21 100\n30 100\n12 100\n22 100\n7 100\n8 100\n4 100\n20 100\n18 100\n14 100\n8 100\n20 100\n16 100\n7 100\n23 100\n24 100\n36 100\n6 100\n7 100\n12 100\n9 100\n4 100\n6 100\n8 100\n10 100\n22 100\n25 100\n24 100\n21 100\n5 100\n5 100\n25 100\n7 100\n7 100\n9 100\n5 100\n28 100\n6 100\n2 100\n14 100\n8 100\n15 100\n8 100\n30 100\n5 100\n6 100\n25 100\n12 100\n8 100\n19 100\n17 100\n14 100\n14 100\n27 100\n6 100\n9 100\n6 100\n22 100\n22 100\n29 100\n9 100\n3 100\n8 100\n9 100\n16 100\n18 100\n6 100\n5 100\n16 100\n28 100\n18 100\n26 100\n9 100\n19 100\n9 100\n6 100\n5 100\n25 100\n5 100\n5 100\n3 100\n16 100\n9 100\n15 100\n6 100\n8 100\n15 100\n21 100\n11 100\n12 100\n27 100\n11 100\n30 100\n6 100\n17 100\n30 100\n4 100\n2 100\n16 100\n23 100\n25 100\n21 100\n8 100\n26 100\n10 100\n10 100\n10 100\n5 100\n6 100\n13 100\n13 100\n38 100\n5 100\n12 100\n9 100\n21 100\n5 100\n20 100\n9 100\n13 100\n9 100\n5 100\n23 100\n8 100\n11 100\n4 100\n25 100\n5 100\n24 100\n6 100\n16 100\n22 100\n17 100\n5 100\n23 100\n21 100\n16 100\n12 100\n12 100\n9 100\n20 100\n24 100\n31 100\n20 100\n25 100\n12 100\n7 100\n11 100\n5 100\n27 100\n27 100\n28 100\n13 100\n14 100\n17 100\n4 100\n21 100\n26 100\n4 100\n16 100\n14 100\n9 100\n22 100\n13 100\n3 100\n16 100\n17 100\n8 100\n10 100\n7 100\n23 100\n5 100\n21 100\n12 100\n8 100\n11 100\n27 100\n6 100\n10 100\n4 100\n42 100\n8 100\n10 100\n7 100\n14 100\n15 100\n6 100\n14 100\n5 100\n10 100\n8 100\n9 100\n13 100\n11 100\n25 100\n14 100\n7 100\n31 100\n4 100\n22 100\n5 100\n25 100\n19 100\n22 100\n5 100\n23 100\n9 100\n15 100\n9 100\n6 100\n6 100\n36 100\n18 100\n27 100\n7 100\n14 100\n21 100\n21 100\n3 100\n23 100\n13 100\n26 100\n6 100\n10 100\n6 100\n6 100\n24 100\n13 100\n3 100\n27 100\n7 100\n4 100\n28 100\n14 100\n25 100\n15 100\n30 100\n8 100\n24 100\n9 100\n34 100\n12 100\n25 100\n24 100\n6 100\n9 100\n8 100\n29 100\n17 100\n5 100\n11 100\n5 100\n4 100\n16 100\n6 100\n11 100\n27 100\n8 100\n30 100\n4 100\n7 100\n8 100\n11 100\n13 100\n4 100\n9 100\n23 100\n17 100\n19 100\n10 100\n7 100\n28 100\n31 100\n5 100\n4 100\n4 100\n9 100\n22 100\n17 100\n5 100\n13 100\n13 100\n14 100\n6 100\n4 100\n6 100\n8 100\n23 100\n24 100\n20 100\n5 100\n29 100\n4 100\n6 100\n20 100\n4 100\n11 100\n6 100\n17 100\n21 100\n19 100\n13 100\n4 100\n6 100\n5 100\n18 100\n8 100\n9 100\n23 100\n32 100\n12 100\n8 100\n12 100\n18 100\n18 100\n31 100\n5 100\n10 100\n38 100\n4 100\n24 100\n23 100\n14 100\n19 100\n5 100\n14 100\n4 100\n25 100\n9 100\n30 100\n23 100\n11 100\n8 100\n7 100\n21 100\n23 100\n4 100\n3 100\n16 100\n26 100\n12 100\n27 100\n9 100\n7 100\n4 100\n2 100\n14 100\n26 100\n16 100\n17 100\n7 100\n2 100\n58 100\n11 100\n17 100\n4 100\n10 100\n12 100\n11 100\n19 100\n12 100\n7 100\n9 100\n23 100\n6 100\n60 100\n7 100\n11 100\n4 100\n8 100\n2 100\n13 100\n14 100\n14 100\n8 100\n22 100\n5 100\n29 100\n19 100\n8 100\n7 100\n6 100\n21 100\n5 100\n6 100\n24 100\n9 100\n9 100\n21 100\n14 100\n17 100\n6 100\n4 100\n13 100\n5 100\n12 100\n5 100\n30 100\n20 100\n7 100\n26 100\n6 100\n11 100\n6 100\n9 100\n9 100\n9 100\n6 100\n12 100\n9 100\n21 100\n25 100\n16 100\n27 100\n14 100\n14 100\n30 100\n10 100\n1 100\n5 100\n6 100\n22 100\n51 100\n6 100\n13 100\n5 100\n6 100\n12 100\n8 100\n3 100\n4 100\n18 100\n13 100\n31 100\n19 100\n8 100\n4 100\n30 100\n13 100\n1 100\n1 100\n5 100\n8 100\n11 100\n14 100\n4 100\n12 100\n9 100\n13 100\n16 100\n6 100\n7 100\n10 100\n5 100\n13 100\n11 100\n11 100\n7 100\n6 100\n6 100\n29 100\n18 100\n5 100\n22 100\n19 100\n13 100\n28 100\n5 100\n11 100\n6 100\n20 100\n33 100\n12 100\n26 100\n5 100\n9 100\n23 100\n19 100\n23 100\n7 100\n23 100\n26 100\n17 100\n9 100\n5 100\n6 100\n16 100\n9 100\n8 100\n5 100\n6 100\n28 100\n8 100\n5 100\n14 100\n4 100\n9 100\n16 100\n12 100\n5 100\n8 100\n22 100\n3 100\n10 100\n12 100\n2 100\n46 100\n5 100\n6 100\n6 100\n25 100\n11 100\n13 100\n23 100\n5 100\n10 100\n4 100\n12 100\n15 100\n22 100\n8 100\n28 100\n9 100\n1 100\n21 100\n29 100\n14 100\n8 100\n9 100\n8 100\n29 100\n4 100\n5 100\n7 100\n8 100\n6 100\n11 100\n10 100\n30 100\n12 100\n26 100\n23 100\n5 100\n27 100\n11 100\n10 100\n4 100\n9 100\n11 100\n5 100\n13 100\n18 100\n19 100\n7 100\n7 100\n7 100\n5 100\n6 100\n6 100\n11 100\n15 100\n9 100\n8 100\n7 100\n6 100\n9 100\n5 100\n8 100\n11 100\n10 100\n14 100\n5 100\n18 100\n24 100\n22 100\n26 100\n11 100\n30 100\n7 100\n22 100\n26 100\n25 100\n9 100\n13 100\n11 100\n8 100\n12 100\n6 100\n8 100\n11 100\n16 100\n15 100\n5 100\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(w2v_vect_avg):\n",
    "    print(len(X_test.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('../data/spam.csv', encoding='latin-1')\n",
    "\n",
    "messages = messages.drop(labels = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\n",
    "messages.columns = ['label', 'text']\n",
    "\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'], messages['label'], test_size = .2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_docs = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TaggedDocument(words=['ranjith', 'cal', 'drpd', 'deeraj', 'and', 'deepak', 'min', 'hold'], tags=[0])"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "tagged_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = gensim.models.Doc2Vec(tagged_docs, vector_size = 100, window = 5, min_count = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.0024029 , -0.0058388 , -0.0005007 ,  0.00502572, -0.0274045 ,\n",
       "        0.00422417,  0.01087392, -0.00486382,  0.00400582,  0.00317663,\n",
       "       -0.0155356 ,  0.00870859,  0.00249349,  0.00909022, -0.01894418,\n",
       "       -0.00112502, -0.00797662, -0.01180502, -0.01751181, -0.00992527,\n",
       "       -0.01334384,  0.00404176, -0.00517214,  0.00636412,  0.00445412,\n",
       "        0.0078181 ,  0.00393844, -0.00156435,  0.00239625,  0.01151866,\n",
       "        0.00182755,  0.00425971,  0.00045227,  0.00509931,  0.00833823,\n",
       "        0.0094913 , -0.00058024, -0.00243734,  0.00861443,  0.0120497 ,\n",
       "        0.00595455, -0.00070964,  0.00733312,  0.00763612, -0.01426243,\n",
       "        0.01128078,  0.01086403,  0.00777117, -0.00795457,  0.00180267,\n",
       "        0.00369564,  0.00364714,  0.00615027,  0.01482986,  0.00682704,\n",
       "       -0.00803831,  0.00515813,  0.01098441,  0.00509766, -0.01267549,\n",
       "       -0.00838568, -0.01197765, -0.00565846, -0.00513034, -0.01967524,\n",
       "        0.01613936, -0.0191185 , -0.00195147, -0.00921478, -0.00049667,\n",
       "       -0.00699465,  0.00384424,  0.00207061, -0.00480603, -0.0073022 ,\n",
       "       -0.01200813,  0.00903636,  0.00383278, -0.00550951,  0.01356461,\n",
       "        0.00389417,  0.008177  ,  0.00229206, -0.00385733,  0.00962973,\n",
       "       -0.00861065,  0.01429916, -0.01224951, -0.01882578, -0.01480056,\n",
       "        0.00537547, -0.00594992,  0.00127041,  0.0072498 , -0.0063453 ,\n",
       "        0.00339118, -0.00440174,  0.00578234, -0.01313484,  0.00170565],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "d2v_model.infer_vector(['i', 'am', 'fucking', 'gay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}