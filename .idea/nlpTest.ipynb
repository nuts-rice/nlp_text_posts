{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  \\\n",
       "0   spam   \n",
       "1    ham   \n",
       "2    ham   \n",
       "3    ham   \n",
       "4    ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "messages = pd.read_csv('../data/SMSSpamCollection.tsv', sep= '\\t')\n",
    "messages.columns = [\"labels\", \"text\"]\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  \\\n",
       "0   spam   \n",
       "1    ham   \n",
       "2    ham   \n",
       "3    ham   \n",
       "4    ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "1                                          Nah I dont think he goes to usf he lives around here though  \n",
       "2                          Even my brother is not like to speak with me They treat me like aids patent  \n",
       "3                                                                    I HAVE A DATE ON SUNDAY WITH WILL  \n",
       "4  As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n      <td>As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  \\\n",
       "0   spam   \n",
       "1    ham   \n",
       "2    ham   \n",
       "3    ham   \n",
       "4    ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                            text_clean  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "1                                          Nah I dont think he goes to usf he lives around here though   \n",
       "2                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "3                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "4  As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...   \n",
       "\n",
       "                                                                                        text_tokenized  \n",
       "0  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "1                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "2         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n",
       "3                                                           [i, have, a, date, on, sunday, with, will]  \n",
       "4  [as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n      <th>text_clean</th>\n      <th>text_tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n      <td>[i, have, a, date, on, sunday, with, will]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n      <td>As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...</td>\n      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "messages['text_tokenized'] = messages['text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  \\\n",
       "0   spam   \n",
       "1    ham   \n",
       "2    ham   \n",
       "3    ham   \n",
       "4    ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                            text_clean  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "1                                          Nah I dont think he goes to usf he lives around here though   \n",
       "2                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "3                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "4  As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...   \n",
       "\n",
       "                                                                                        text_tokenized  \\\n",
       "0  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "1                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "2         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "3                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "4  [as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...   \n",
       "\n",
       "                                                                                           text_nostop  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n      <th>text_clean</th>\n      <th>text_tokenized</th>\n      <th>text_nostop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>Nah I dont think he goes to usf he lives around here though</td>\n      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n      <td>[i, have, a, date, on, sunday, with, will]</td>\n      <td>[date, sunday]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n      <td>As per your request Melle Melle Oru Minnaminunginte Nurungu Vettam has been set as your callertu...</td>\n      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "messages['text_nostop'] = messages['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def clean_text(text):\n",
    "     text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "     tokens = re.split('\\W+', text)\n",
    "     text = [word for word in tokens if word not in stopwords]\n",
    "     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mear', 'smell', 'smells', 'smeone', 'smidgin', 'smile', 'smiled', 'smiles', 'smiley', 'smiling', 'smith', 'smithswitch', 'smoke', 'smoked', 'smokes', 'smokin', 'smoking', 'smoothly', 'sms', 'sms08718727870', 'smsd', 'smsing', 'smsservices', 'smsshsexnetun', 'smth', 'sn', 'snake', 'snap', 'snappy', 'snatch', 'snd', 'sneham', 'snickering', 'sno', 'snogs', 'snoringthey', 'snow', 'snowball', 'snowboarding', 'snowman', 'snuggles', 'soany', 'soc', 'sochte', 'social', 'sofa', 'soft', 'software', 'soil', 'soiree', 'sol', 'soladha', 'sold', 'solihull', 'solve', 'solved', 'some1', 'somebody', 'someday', 'someone', 'someones', 'someonethat', 'someonone', 'someplace', 'somerset', 'somethin', 'something', 'somethings', 'sometime', 'sometimerakheshvisitor', 'sometimes', 'sometme', 'somewhat', 'somewhere', 'somewheresomeone', 'somewhr', 'somone', 'somtimes', 'sonathaya', 'sonetimes', 'song', 'songs', 'sonot', 'sony', 'sonyericsson', 'soo', 'soon', 'soonc', 'sooner', 'soonlots', 'soonxxx', 'sooo', 'soooo', 'sooooo', 'sophas', 'sore', 'sorrow', 'sorrowsi', 'sorry', 'sorryi', 'sorryin', 'sort', 'sorta', 'sorted', 'sortedbut', 'sorting', 'sorts', 'sory', 'sorydarealyfrm', 'sos', 'soso', 'soul', 'sound', 'sounding', 'sounds', 'soundtrack', 'soup', 'source', 'sources', 'south', 'southern', 'souveniers', 'soz', 'sp', 'space', 'spacebucks', 'spaces', 'spageddies', 'spain', 'spam', 'spanish', 'spare', 'spares', 'spark', 'sparkling', 'spatula', 'speak', 'speaking', 'special', 'specialcall', 'speciale', 'specialisation', 'specialise', 'specially', 'specific', 'specify', 'specs', 'speechless', 'speed', 'speedchat', 'speeding', 'speling', 'spell', 'spelled', 'spelling', 'spend', 'spending', 'spent', 'spice', 'spider', 'spiderman', 'spiffing', 'spile', 'spin', 'spinout', 'spiral', 'spirit', 'spiritual', 'spjanuary', 'spk', 'spl', 'splash', 'splashmobile', 'splat', 'splendid', 'split', 'splleing', 'splwat', 'spoil', 'spoiled', 'spoilt', 'spoke', 'spoken', 'sponsors', 'spontaneously', 'spook', 'spoon', 'spoons', 'sporadically', 'sport', 'sports', 'sportsx', 'spose', 'spot', 'spotty', 'spouse', 'sppok', 'spreadsheet', 'spree', 'spring', 'springs', 'sprint', 'sprwm', 'sptv', 'sptyrone', 'spunout', 'spys', 'sq825', 'squatting', 'squeeeeeze', 'squeezed', 'squid', 'squishy', 'srs', 'srsly', 'srt', 'sry', 'ssi', 'ssindia', 'ssnervous', 'st', 'stability', 'stable', 'stadium', 'staff', 'staffsciencenusedusgphyhcmkteachingpc1323', 'stage', 'stagwood', 'stairs', 'stalk', 'stalking', 'stamped', 'stamps', 'stand', 'standard', 'standing', 'stands', 'stapati', 'star', 'starer', 'staring', 'starring', 'stars', 'starshine', 'start', 'started', 'startedindia', 'starti', 'starting', 'starts', 'starve', 'starving', 'starwars3', 'stash', 'stated', 'statement', 'statements', 'station', 'stations', 'status', 'stay', 'stayed', 'stayin', 'staying', 'stays', 'std', 'stdtxtrate', 'steak', 'steal', 'stealing', 'steam', 'steamboat', 'steed', 'steering', 'step', 'steps', 'stereo', 'stereophonics', 'sterling', 'sterm', 'steve', 'stevelike', 'stewartsize', 'steyn', 'sth', 'sthis', 'stick', 'sticky', 'stifled', 'stil', 'still', 'stillmaybe', 'stink', 'stitch', 'stock', 'stocked', 'stockport', 'stolen', 'stomach', 'stomps', 'stone', 'stoners', 'stones', 'stool', 'stop', 'stop2', 'stop2stop', 'stopbcm', 'stopcost', 'stopcs', 'stopped', 'stops', 'stoptxt', 'stoptxtstop', 'store', 'storelike', 'stores', 'stories', 'storming', 'story', 'str', 'str8', 'straight', 'strain', 'strange', 'stranger', 'strangersaw', 'stream', 'street', 'streetshall', 'stress', 'stressed', 'stressful', 'stressfull', 'stretch', 'strewn', 'strict', 'strike', 'strings', 'strip', 'stripes', 'strips', 'strokes', 'strong', 'strongbuy', 'strongly', 'strt', 'strtd', 'struggling', 'sts', 'stu', 'stubborn', 'stuck', 'studdying', 'student', 'studentfinancial', 'students', 'studentsthis', 'studies', 'studio', 'study', 'studying', 'studyn', 'stuff', 'stuff42moro', 'stuffed', 'stuffing', 'stuffleaving', 'stuffs', 'stuffwhy', 'stunning', 'stupid', 'stupidits', 'style', 'styles', 'styling', 'stylish', 'stylist', 'sub', 'subject', 'subletting', 'submitted', 'submitting', 'subpoly', 'subs', 'subscribe', 'subscribe6gbpmnth', 'subscribed', 'subscriber', 'subscribers', 'subscription', 'subscriptions', 'subscriptn3gbpwk', 'subscrition', 'subsequent', 'subtoitles', 'success', 'successful', 'successfully', 'sucker', 'suckers', 'sucks', 'sudden', 'suddenly', 'sudn', 'sue', 'suffer', 'suffering', 'suffers', 'sufficient', 'sugababes', 'suganya', 'sugar', 'sugardad', 'suggest', 'suggestion', 'suggestions', 'suite', 'suitemates', 'suits', 'sullivan', 'sum', 'sum1', 'sumfing', 'summer', 'summers', 'summon', 'sumthin', 'sumthinxx', 'sun', 'sun0819', 'sunday', 'sundayish', 'sunlight', 'sunny', 'sunoco', 'sunroof', 'sunscreen', 'sunshine', 'suntec', 'sup', 'super', 'superb', 'superior', 'supervisor', 'suply', 'supose', 'suppliers', 'supplies', 'supply', 'support', 'supportproviding', 'supports', 'supportvery', 'suppose', 'supposed', 'supreme', 'suprman', 'sura', 'sure', 'surely', 'surf', 'surfing', 'surgical', 'surly', 'surname', 'surprise', 'surprised', 'surrender', 'surrounded', 'survey', 'surya', 'sutra', 'sux', 'suzy', 'svc', 'sw7', 'sw73ss', 'swalpa', 'swan', 'swann', 'swap', 'swashbuckling', 'swat', 'swatch', 'sway', 'swayze', 'swear', 'sweater', 'sweatter', 'sweet', 'sweetest', 'sweetheart', 'sweetie', 'sweets', 'swell', 'swhrt', 'swimming', 'swimsuit', 'swing', 'swiss', 'switch', 'swollen', 'swoop', 'swt', 'swtheart', 'syd', 'syllabus', 'symbol', 'sympathetic', 'symptoms', 'synced', 'syria', 'syrup', 'system', 'systems', 't91', 'ta', 'table', 'tables', 'tablet', 'tablets', 'tackle', 'tacos', 'tactful', 'tactless', 'tadaaaaa', 'tag', 'tagged', 'tahan', 'tai', 'tait', 'taj', 'taka', 'take', 'takecare', 'taken', 'takenonly', 'takes', 'takin', 'taking', 'talent', 'talents', 'talk', 'talkbut', 'talked', 'talkin', 'talking', 'talks', 'tall', 'tallahassee', 'tallent', 'tamilnaduthen', 'tampa', 'tank', 'tantrums', 'tap', 'tape', 'tariffs', 'tarot', 'tarpon', 'tas', 'taste', 'tasts', 'tat', 'tata', 'tattoos', 'tau', 'taught', 'taunton', 'taxes', 'taxi', 'taxless', 'taxt', 'taylor', 'taylors', 'tayseertissco', 'tb', 'tbspersolvo', 'tc', 'tcllc', 'tcrw1', 'tcs', 'tcsbcm4235wc1n3xx', 'tcsc', 'tcsstop', 'tddnewsletteremc1couk', 'tea', 'teach', 'teacher', 'teaches', 'teaching', 'teacoffee', 'team', 'teams', 'tear', 'tears', 'tease', 'teasing', 'tech', 'technical', 'technologies', 'tee', 'teenager', 'teeth', 'teethif', 'teethis', 'teju', 'tel', 'telephone', 'telephonic', 'teletext', 'tell', 'telling', 'tellmiss', 'tells', 'telly', 'telphone', 'telugu', 'teluguthts', 'temales', 'temp', 'temper', 'temple', 'ten', 'tenants', 'tendencies', 'tenerife', 'tensed', 'tension', 'teresa', 'term', 'terminatedwe', 'terms', 'termsapply', 'terrible', 'terrific', 'terrorist', 'terry', 'tescos', 'tessypls', 'test', 'testing', 'tests', 'tex', 'texas', 'texd', 'text', 'text82228', 'textand', 'textbook', 'textbuddy', 'textcomp', 'texted', 'textin', 'texting', 'textoperator', 'textpod', 'texts', 'textsweekend', 'tgxxrz', 'th', 'thandiyachu', 'thangam', 'thangamits', 'thank', 'thanks', 'thanks2', 'thanksgiving', 'thanku', 'thankyou', 'thanx', 'thanx4', 'thanxxx', 'thasa', 'that2worzels', 'thatd', 'thatdont', 'thati', 'thatll', 'thatmum', 'thatnow', 'thats', 'the4th', 'theacusations', 'theater', 'theatre', 'thecd', 'thedailydraw', 'thekingshead', 'themed', 'themes', 'themob', 'themobhit', 'themobyo', 'themp', 'thenwill', 'theoretically', 'theory', 'theplace', 'thepub', 'theredo', 'theregoodnight', 'therell', 'therere', 'theres', 'therexx', 'thesedays', 'theseyours', 'thesis', 'thesmszonecom', 'thewend', 'theyll', 'theyre', 'thgt', 'thia', 'thin', 'thing', 'thinghow', 'things', 'think', 'thinked', 'thinkin', 'thinking', 'thinks', 'thinkthis', 'thinl', 'thirunelvali', 'thisdon', 'thk', 'thkin', 'thm', 'thnk', 'thnq', 'thnx', 'tho', 'thoso', 'thot', 'thou', 'though', 'thought', 'thoughts', 'thoughtsi', 'thousands', 'thout', 'thread', 'threats', 'three', 'threw', 'thriller', 'throat', 'throw', 'throwin', 'throwing', 'thrown', 'throws', 'thru', 'thrurespect', 'ths', 'tht', 'thts', 'thuglyfe', 'thurs', 'thursday', 'thus', 'thx', 'thy', 'tick', 'ticket', 'tickets', 'tiempo', 'tiger', 'tight', 'tightly', 'tigress', 'tihs', 'tiime', 'til', 'till', 'tim', 'time', 'timedhoni', 'timegud', 'timehope', 'times', 'timeslil', 'timeyou', 'timeyour', 'timi', 'timin', 'timing', 'timings', 'tiny', 'tip', 'tips', 'tired', 'tiring', 'tirunelvai', 'tirunelvali', 'tirupur', 'tis', 'tisscotayseer', 'title', 'titles', 'titleso', 'tiwary', 'tix', 'tiz', 'tke', 'tkts', 'tlk', 'tm', 'tming', 'tmobile', 'tmorrowpls', 'tmr', 'tmrw', 'tmw', 'tnc', 'tncs', 'toa', 'toaday', 'tobacco', 'tobed', 'tocallshall', 'toclaim', 'today', 'todaybut', 'todaydo', 'todayfrom', 'todaygood', 'todayhe', 'todays', 'todaysundaysunday', 'todo', 'tog', 'together', 'tohar', 'toilet', 'tok', 'token', 'toking', 'tol', 'told', 'toldshe', 'toledo', 'tolerance', 'toleratbcs', 'toll', 'tom', 'tomarrow', 'tome', 'tomeandsaidthis', 'tomo', 'tomocant', 'tomorro', 'tomorrow', 'tomorrowcall', 'tomorrowtoday', 'tomorw', 'tone', 'tones', 'tones2u', 'tones2youcouk', 'tonesreply', 'tonexs', 'tonght', 'tongued', 'tonight', 'tonights', 'tonite', 'tonitebusy', 'tonitethings', 'tons', 'tonsolitusaswell', 'took', 'tookplace', 'tool', 'toolets', 'tooo', 'toopray', 'toot', 'toothpaste', 'tootsie', 'top', 'topic', 'topicsorry', 'toplay', 'topped', 'toppoly', 'tops', 'tor', 'torch', 'torrents', 'tortilla', 'torture', 'tosend', 'toshiba', 'toss', 'tot', 'total', 'totally', 'totes', 'touch', 'touched', 'tough', 'toughest', 'tour', 'towards', 'town', 'towncud', 'towndontmatter', 'toxic', 'toyota', 'tp', 'track', 'trackmarque', 'trade', 'traditions', 'traffic', 'train', 'trained', 'training', 'trainners', 'trains', 'tram', 'tranquility', 'transaction', 'transcribing', 'transfer', 'transferacc', 'transfered', 'transferred', 'transfr', 'transfred', 'transport', 'trash', 'trauma', 'trav', 'travel', 'traveling', 'travelled', 'travelling', 'treacle', 'treadmill', 'treasure', 'treat', 'treated', 'treatin', 'treats', 'trebles', 'tree', 'trek', 'trends', 'trial', 'tried', 'trip', 'triple', 'trips', 'trishul', 'triumphed', 'tron', 'trouble', 'troubleshooting', 'trouser', 'truble', 'truck', 'true', 'truekdo', 'truffles', 'truly', 'truro', 'trust', 'trusting', 'truth', 'truthful', 'try', 'tryin', 'trying', 'trywales', 'ts', 'tsandcs', 'tscs', 'tscs08714740323', 'tscs087147403231winawkage16', 'tshirt', 'tsunami', 'tsunamis', 'tt', 'tts', 'ttyl', 'tue', 'tues', 'tuesday', 'tui', 'tuition', 'tul', 'tulip', 'tunde', 'tune', 'tunji', 'turkeys', 'turn', 'turned', 'turning', 'turns', 'tuth', 'tv', 'tvhe', 'tvlol', 'twat', 'twelve', 'twenty', 'twice', 'twiggs', 'twilight', 'twinks', 'twins', 'twittering', 'two', 'tx', 'txt', 'txt250com', 'txtauction', 'txtauctiontxt', 'txtin', 'txting', 'txtjourney', 'txtno', 'txts', 'txtx', 'tyler', 'tylers', 'type', 'typelyk', 'types', 'typical', 'u', 'u2moro', 'u4', 'uawakefeellikw', 'ubandu', 'ubi', 'ucall', 'ufind', 'ugadi', 'ugh', 'ugos', 'uh', 'uhhhhrmm', 'uif', 'uin', 'ujhhhhhhh', 'uk', 'ukmobiledate', 'ukp2000', 'uks', 'ull', 'ultimate', 'ultimately', 'ultimatum', 'um', 'umma', 'ummmawill', 'ummmmmaah', 'un', 'unable', 'unbelievable', 'unclaimed', 'uncle', 'uncles', 'uncomfortable', 'unconditionally', 'unconscious', 'unconsciously', 'unconvinced', 'uncountable', 'uncut', 'underdtand', 'understand', 'understanding', 'understood', 'underwear', 'undrstnd', 'undrstndng', 'unemployed', 'uneventful', 'unfolds', 'unfortunately', 'unfortuntly', 'unhappiness', 'unhappy', 'uni', 'unicefs', 'uniform', 'unintentional', 'unintentionally', 'unique', 'uniquei', 'united', 'units', 'univ', 'university', 'unknown', 'unless', 'unlike', 'unlimited', 'unmits', 'unnecessarily', 'unni', 'unrecognized', 'unredeemed', 'unsecured', 'unsold', 'unsoldmike', 'unsoldnow', 'unspoken', 'unsub', 'unsubscribe', 'unsubscribed', 'unusual', 'uothrwise', 'up4', 'upcharge', 'upd8', 'updat', 'update', 'updatenow', 'upgrade', 'upgrading', 'upgrdcentre', 'uphad', 'upload', 'uploaded', 'uploads', 'upnot', 'upon', 'upping', 'ups', 'upset', 'upseti', 'upsetits', 'upstairs', 'upto', 'uptown', 'upyeh', 'ur', 'ure', 'urfeeling', 'urgent', 'urgentbut', 'urgentlyits', 'urgh', 'urgnt', 'urgoin', 'urgran', 'urination', 'url', 'urmomi', 'urn', 'urself', 'us', 'usb', 'usc', 'uscedu', 'use', 'used', 'useful', 'useless', 'user', 'uses', 'usf', 'usget', 'usher', 'using', 'uslet', 'usmle', 'usno', 'uso', 'usps', 'usual', 'usualiam', 'usually', 'uterus', 'utter', 'uttered', 'uu', 'uup', 'uv', 'uve', 'uwana', 'uwant', 'uworld', 'uxxxx', 'v', 'vaazhthukkal', 'vague', 'vaguely', 'vai', 'vale', 'valentine', 'valentines', 'valid', 'valid12hrs', 'valuable', 'value', 'valued', 'valuemorning', 'values', 'valuing', 'varaya', 'vargu', 'various', 'varma', 'vary', 'vasai', 'vat', 'vatian', 'vava', 'vco', 'vday', 'vegas', 'vegetables', 'veggie', 'vehicle', 'velachery', 'velly', 'velusamy', 'venaam', 'venugopal', 'verified', 'verify', 'verifying', 'version', 'versus', 'vettam', 'vewy', 'via', 'vibrant', 'vibrate', 'vibrator', 'vic', 'victoria', 'victors', 'vid', 'video', 'videochat', 'videophones', 'videopic', 'videos', 'videosound', 'videosounds2', 'vidnot', 'view', 'vijay', 'vijaykanth', 'vikky', 'vikkyim', 'vilikkamt', 'vill', 'villa', 'village', 'vinobanagar', 'violated', 'violence', 'violet', 'vip', 'virgils', 'virgin', 'virgins', 'virtual', 'visa', 'visionsmscom', 'visit', 'visiting', 'visitneed', 'visitors', 'vital', 'vitamin', 'viva', 'vivek', 'vivekanand', 'viveki', 'vl', 'vldo', 'voda', 'vodafone', 'vodka', 'voice', 'voicemail', 'voila', 'volcanoes', 'vomit', 'vomitin', 'vomiting', 'vote', 'voted', 'voucher', 'vouchers', 'voucherstext', 'vpist', 'vpod', 'vry', 'vs', 'vth', 'vtired', 'vu', 'w', 'w111wx', 'w14rg', 'w1a', 'w1j', 'w1t1jy', 'w4', 'w45wq', 'w8in', 'wa', 'wa14', 'waaaat', 'wad', 'wadebridgei', 'wah', 'wahala', 'wahay', 'waheed', 'waheeda', 'wahleykkumsharing', 'waht', 'wait', 'waited', 'waiti', 'waitin', 'waiting', 'waitshould', 'waitu', 'wake', 'waking', 'wales', 'waliking', 'walk', 'walkabout', 'walked', 'walkin', 'walking', 'walks', 'wall', 'wallet', 'wallpaper', 'wallpaperall', 'walls', 'walmart', 'walsall', 'wamma', 'wan', 'wan2', 'wana', 'wanna', 'wannatell', 'want', 'want2come', 'wanted', 'wanting', 'wants', 'wap', 'waqt', 'warm', 'warming', 'warned', 'warner', 'warning', 'warranty', 'warwick', 'washob', 'wasnt', 'waste', 'wasted', 'wasting', 'wat', 'watch', 'watched', 'watches', 'watchin', 'watching', 'watchng', 'water', 'watever', 'watevr', 'wating', 'watll', 'watrdayno', 'wats', 'watts', 'waves', 'way', 'way2smscom', 'waythis', 'wc', 'wc1n', 'wc1n3xx', 'weak', 'weakness', 'weaknesses', 'weapon', 'wear', 'wearing', 'weaseling', 'weasels', 'weather', 'weathers', 'web', 'web2mobile', 'webadres', 'webeburnin', 'webpage', 'website', 'websitenow', 'wed', 'weddin', 'wedding', 'weddingfriend', 'wedlunch', 'wednesday', 'weds', 'wee', 'weed', 'weeddeficient', 'week', 'weekdays', 'weekend', 'weekends', 'weekly', 'weeks', 'weekstop', 'weigh', 'weighed', 'weight', 'weighthaha', 'weightloss', 'weird', 'weirdest', 'weirdo', 'weirdy', 'weiyi', 'welcome', 'welcomes', 'well', 'wellda', 'welli', 'welltake', 'wellyou', 'welp', 'wen', 'wendy', 'wenever', 'went', 'wenwecan', 'wer', 'wereare', 'werebored', 'werent', 'werethe', 'wesley', 'wesleys', 'west', 'western', 'westlife', 'westonzoyland', 'westshore', 'wet', 'wetherspoons', 'weve', 'wewa', 'whassup', 'whatever', 'whats', 'whatsup', 'wheat', 'wheel', 'wheellock', 'whenever', 'whenevr', 'whenre', 'whens', 'whenwhere', 'whereare', 'wherebtw', 'wheres', 'wherever', 'wherevr', 'wherres', 'whether', 'whileamp', 'whilltake', 'whispers', 'white', 'whn', 'whole', 'whore', 'whos', 'whose', 'whr', 'wi', 'wicked', 'wicket', 'wicklow', 'wid', 'widelivecomindex', 'wif', 'wife', 'wifedont', 'wifehow', 'wifes', 'wifi', 'wihtuot', 'wikipediacom', 'wil', 'wild', 'wildest', 'wildlife', 'willing', 'willpower', 'win', 'win150ppmx3age16', 'wind', 'window', 'windows', 'winds', 'windy', 'wine', 'wined', 'wings', 'wining', 'winner', 'winnersclub', 'winning', 'wins', 'winterstone', 'wipe', 'wipro', 'wiproyou', 'wire3net', 'wisdom', 'wise', 'wish', 'wisheds', 'wishes', 'wishin', 'wishing', 'wishlist', 'wiskey', 'wit', 'withdraw', 'wither', 'within', 'without', 'witin', 'witot', 'witout', 'wiv', 'wizzle', 'wk', 'wkend', 'wkent150p16', 'wkg', 'wkly', 'wknd', 'wks', 'wktxt', 'wlcome', 'wld', 'wmlid1b6a5ecef91ff937819firsttrue180430jul05', 'wmlid820554ad0a1705572711firsttrue', 'wn', 'wnevr', 'wnt', 'wo', 'woah', 'wocay', 'woke', 'woken', 'woman', 'womdarfull', 'women', 'wondar', 'wondarfull', 'wonder', 'wonderful', 'wondering', 'wonders', 'wont', 'woo', 'woodland', 'woods', 'woohoo', 'woot', 'woould', 'woozles', 'worc', 'word', 'wordcollect', 'wordnot', 'words', 'wordsevry', 'wordstart', 'work', 'workage', 'workand', 'workin', 'working', 'worklove', 'workout', 'works', 'world', 'worldgnun', 'worldmay', 'worlds', 'worldvery', 'worms', 'worried', 'worriedx', 'worries', 'worry', 'worryc', 'worrying', 'worryuse', 'worse', 'worst', 'worth', 'worthless', 'wot', 'wotu', 'wotz', 'woul', 'would', 'woulda', 'wouldnt', 'wounds', 'wow', 'wquestion', 'wrc', 'wrecked', 'wrench', 'wrenching', 'wright', 'write', 'writhing', 'wrk', 'wrki', 'wrkin', 'wrking', 'wrks', 'wrld', 'wrnog', 'wrong', 'wrongly', 'wrongtake', 'wrote', 'ws', 'wt', 'wtc', 'wtf', 'wth', 'wthout', 'wud', 'wudnt', 'wuld', 'wuldnt', 'wun', 'www07781482378com', 'www4tcbiz', 'www80488biz', 'wwwapplausestorecom', 'wwwareyouuniquecouk', 'wwwasjesuscom', 'wwwb4utelecom', 'wwwbridalpetticoatdreamscouk', 'wwwcashbincouk', 'wwwclubmobycom', 'wwwclubzedcouk', 'wwwcnupdatescomnewsletter', 'wwwcomuknet', 'wwwdbuknet', 'wwwflirtpartyus', 'wwwfullonsmscom', 'wwwgambtv', 'wwwgetzedcouk', 'wwwidewcom', 'wwwldewcom', 'wwwldewcom1win150ppmx3age16', 'wwwldewcom1win150ppmx3age16subscription', 'wwwldewcomsubs161win150ppmx3', 'wwwmovietriviatv', 'wwwmusictrivianet', 'wwworangecoukow', 'wwwphb1com', 'wwwregalportfoliocouk', 'wwwringtonekingcouk', 'wwwringtonescouk', 'wwwrtfsphostingcom', 'wwwsantacallingcom', 'wwwshortbreaksorguk', 'wwwsmsacubootydelious', 'wwwsmsacugoldviking', 'wwwsmsacuhmmross', 'wwwsmsacunat27081980', 'wwwsmsacunatalie2k9', 'wwwsmsconet', 'wwwtcbiz', 'wwwtelediscountcouk', 'wwwtextcompcom', 'wwwtextpodnet', 'wwwtklscom', 'wwwtxt2shopcom', 'wwwtxt43com', 'wwwtxt82228com', 'wwwtxttowincouk', 'wwwwin82050couk', 'wylie', 'x', 'x2', 'x29', 'x49', 'x49your', 'xafter', 'xam', 'xavier', 'xchat', 'xclusiveclubsaisai', 'xin', 'xins', 'xmas', 'xnet', 'xoxo', 'xt', 'xuhui', 'xx', 'xxsp', 'xxuk', 'xxx', 'xxxmobilemovieclub', 'xxxmobilemovieclubcomnqjkgighjjgcbl', 'xxxx', 'xxxxx', 'xxxxxx', 'xxxxxxx', 'xxxxxxxx', 'xxxxxxxxxxxxxx', 'xy', 'y87', 'ya', 'yagoing', 'yah', 'yahoo', 'yalrigu', 'yalru', 'yam', 'yan', 'yar', 'yards', 'yavnt', 'yaxx', 'yaxxx', 'yay', 'yck', 'yday', 'yeah', 'yeahand', 'year', 'years', 'yeesh', 'yeh', 'yelling', 'yellow', 'yelowi', 'yen', 'yeovil', 'yep', 'yer', 'yes', 'yes165', 'yes434', 'yes440', 'yes762', 'yes910', 'yesbut', 'yesfrom', 'yesgauti', 'yeshe', 'yeshere', 'yesim', 'yesmum', 'yessura', 'yest', 'yesterday', 'yet', 'yettys', 'yetunde', 'yi', 'yifeng', 'yijue', 'yijuehotmailcom', 'ym', 'ymca', 'yo', 'yoga', 'yogasana', 'yohere', 'yor', 'yorge', 'youany', 'youcarlos', 'youclean', 'youd', 'youdearwith', 'youdoing', 'youhow', 'youi', 'youkwhere', 'yould', 'youll', 'youmoney', 'youmy', 'young', 'younger', 'youphone', 'youre', 'yourinclusive', 'yourjob', 'youso', 'youthats', 'youto', 'youuuuu', 'youve', 'youwanna', 'youwhen', 'yoville', 'yowifes', 'yoyyooo', 'yr', 'yrs', 'ystrdayice', 'yummmm', 'yummy', 'yun', 'yunny', 'yuo', 'yuou', 'yup', 'yupz', 'ywhere', 'z', 'zac', 'zahers', 'zealand', 'zebra', 'zed', 'zeros', 'zhong', 'zindgi', 'zoe', 'zogtorius', 'zoom', 'zouk', 'zyada', 'é', 'ü', 'üll', '〨ud']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfid_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "X_tfidf = tfid_vect.fit_transform(messages['text'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfid_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = pd.DataFrame(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, messages['labels'], test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 1.0 / Recall: 0.842\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "print('Precision: {} / Recall: {}'.format(round(precision, 3), round(recall, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already up-to-date: gensim in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from gensim) (4.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/mishkasistrunk/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "wiki_embeddings = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "wiki_embeddings['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('prince', 0.7682329416275024),\n",
       " ('queen', 0.7507690191268921),\n",
       " ('son', 0.7020887136459351),\n",
       " ('brother', 0.6985775232315063),\n",
       " ('monarch', 0.6977890729904175),\n",
       " ('throne', 0.6919990181922913),\n",
       " ('kingdom', 0.6811410188674927),\n",
       " ('father', 0.6802029013633728),\n",
       " ('emperor', 0.6712857484817505),\n",
       " ('ii', 0.6676074266433716)]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "wiki_embeddings.most_similar('king')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  labels  \\\n",
       "0   spam   \n",
       "1    ham   \n",
       "2    ham   \n",
       "3    ham   \n",
       "4    ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                            text_clean  \\\n",
       "0  [free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...   \n",
       "1                                [nah, don, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "2         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "3                                                                 [have, date, on, sunday, with, will]   \n",
       "4  [as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...   \n",
       "\n",
       "                                                                                        text_tokenized  \\\n",
       "0  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "1                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "2         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "3                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "4  [as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...   \n",
       "\n",
       "                                                                                           text_nostop  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n      <th>text_clean</th>\n      <th>text_tokenized</th>\n      <th>text_nostop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n      <td>[have, date, on, sunday, with, will]</td>\n      <td>[i, have, a, date, on, sunday, with, will]</td>\n      <td>[date, sunday]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n      <td>[as, per, your, request, melle, melle, oru, minnaminunginte, nurungu, vettam, has, been, set, as...</td>\n      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'], messages['labels'], test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                size = 100,\n",
    "                                window = 5,\n",
    "                                min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.02385644,  0.02702795, -0.04689848,  0.01321273,  0.00371482,\n",
       "        0.04875063,  0.02298244,  0.01416697,  0.04908756,  0.01059419,\n",
       "       -0.0167664 , -0.01892219,  0.0358916 , -0.02114569,  0.00766758,\n",
       "       -0.00090847,  0.00240352,  0.05769638, -0.02899728,  0.03632395,\n",
       "        0.02055868,  0.03291567, -0.01237348,  0.03144512,  0.0628041 ,\n",
       "       -0.04568134,  0.01124457, -0.00712381, -0.03204733,  0.00825624,\n",
       "        0.00645503, -0.01473418, -0.02594489, -0.05524549,  0.01920295,\n",
       "       -0.02673784, -0.0316071 , -0.01360109, -0.06399588, -0.04358508,\n",
       "        0.03401199,  0.01947162, -0.02558148, -0.03108014,  0.00993923,\n",
       "       -0.04614263,  0.03764139, -0.01310852, -0.00367732,  0.03164873,\n",
       "        0.01324575,  0.04776942, -0.00015941,  0.01856351,  0.03619543,\n",
       "       -0.05212099, -0.00265372,  0.03775365, -0.05088244,  0.00414503,\n",
       "       -0.02044275,  0.02428108, -0.00929672,  0.07851196, -0.00882286,\n",
       "       -0.05823258, -0.00290163,  0.10997419, -0.01644889,  0.02953064,\n",
       "       -0.05506734,  0.01300488, -0.06838337,  0.01444352, -0.08192733,\n",
       "        0.03626653, -0.01794342,  0.05944977, -0.00723931, -0.00970822,\n",
       "        0.01636853,  0.00114536, -0.01284214,  0.07989796, -0.08781932,\n",
       "       -0.03427755, -0.04900708, -0.02967659, -0.00027154, -0.0255814 ,\n",
       "       -0.01147556,  0.01898661,  0.06329172,  0.1042525 , -0.01873505,\n",
       "        0.05278723, -0.00644246,  0.00760515, -0.06301706,  0.00035003],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "w2v_model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('smile', 0.9975340366363525),\n",
       " ('messages', 0.9975086450576782),\n",
       " ('use', 0.9975019693374634),\n",
       " ('maybe', 0.9974759817123413),\n",
       " ('lot', 0.9974648952484131),\n",
       " ('offer', 0.9974614381790161),\n",
       " ('next', 0.997458815574646),\n",
       " ('to', 0.9974520206451416),\n",
       " ('always', 0.9974443912506104),\n",
       " ('also', 0.9974367618560791)]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in w2v_model.wv.index2word]) for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11 11\n5 4\n6 6\n26 22\n22 22\n5 5\n3 3\n6 6\n5 4\n25 25\n20 20\n8 8\n8 6\n7 7\n31 25\n19 13\n26 26\n11 10\n7 5\n7 5\n46 40\n6 6\n7 7\n6 6\n13 13\n12 10\n6 4\n22 21\n12 11\n7 5\n14 13\n6 5\n9 8\n4 4\n9 9\n5 5\n17 15\n19 16\n20 17\n24 21\n6 6\n7 7\n9 9\n6 6\n22 10\n11 10\n8 7\n25 19\n3 3\n5 5\n23 15\n23 22\n6 6\n8 5\n19 15\n8 8\n6 5\n1 1\n11 11\n11 10\n30 28\n14 11\n7 6\n7 7\n6 6\n17 14\n21 17\n21 21\n9 9\n8 7\n7 6\n6 6\n4 4\n23 22\n8 7\n42 36\n15 14\n26 25\n9 9\n25 25\n81 79\n16 15\n4 4\n8 8\n7 7\n47 32\n7 7\n6 5\n2 2\n7 7\n9 9\n20 20\n24 22\n21 21\n6 6\n31 28\n20 19\n13 12\n9 9\n5 5\n17 17\n13 12\n13 13\n8 7\n15 12\n19 18\n14 11\n14 12\n5 5\n6 6\n5 5\n27 27\n12 12\n7 5\n22 20\n2 2\n14 14\n8 7\n10 9\n13 13\n21 17\n8 7\n6 6\n10 10\n9 8\n7 5\n29 20\n13 7\n11 9\n6 4\n26 23\n32 25\n31 28\n21 15\n8 8\n9 8\n7 6\n14 14\n14 12\n8 5\n9 9\n4 4\n8 8\n29 25\n14 13\n28 28\n10 9\n21 21\n4 4\n47 40\n5 5\n4 3\n5 5\n10 8\n17 16\n1 1\n44 40\n4 4\n6 5\n23 21\n20 18\n18 15\n18 16\n9 9\n12 12\n4 4\n15 14\n9 9\n5 5\n74 66\n14 14\n5 5\n7 7\n176 162\n7 7\n7 7\n24 20\n17 11\n4 4\n10 9\n8 8\n18 17\n30 24\n21 19\n4 4\n12 12\n9 9\n24 24\n25 22\n13 11\n15 14\n4 4\n29 28\n15 13\n5 4\n2 2\n5 5\n11 10\n7 7\n9 9\n25 23\n24 23\n4 4\n17 14\n24 18\n17 14\n110 83\n23 22\n17 17\n6 5\n1 1\n11 11\n6 6\n12 11\n4 4\n7 7\n6 6\n12 12\n7 7\n5 4\n12 12\n24 20\n5 5\n14 12\n7 6\n7 7\n15 15\n9 9\n18 18\n10 10\n6 6\n11 11\n5 5\n20 20\n10 8\n7 7\n8 8\n8 8\n18 18\n6 6\n28 25\n9 8\n5 4\n25 23\n6 6\n5 3\n6 6\n14 11\n12 12\n16 16\n21 21\n11 11\n10 9\n24 19\n24 20\n7 6\n10 9\n17 12\n29 28\n7 7\n6 5\n28 28\n32 31\n26 26\n8 8\n26 22\n12 12\n4 4\n29 29\n10 7\n5 5\n7 5\n8 8\n20 20\n16 14\n9 9\n26 21\n4 4\n7 4\n6 5\n6 5\n23 21\n16 16\n8 7\n27 26\n13 11\n15 14\n8 7\n15 15\n5 5\n27 25\n31 31\n26 25\n8 6\n6 5\n12 12\n13 8\n20 19\n24 20\n23 21\n22 22\n11 10\n14 12\n6 6\n25 23\n3 2\n9 8\n11 10\n12 10\n25 24\n21 20\n25 20\n28 26\n7 7\n17 17\n6 4\n22 22\n22 21\n7 5\n22 21\n5 4\n7 6\n5 5\n25 24\n24 20\n7 5\n18 17\n8 8\n20 17\n22 18\n24 23\n26 24\n23 23\n9 9\n25 25\n12 12\n25 20\n18 14\n23 11\n5 5\n3 3\n16 15\n11 10\n11 10\n6 6\n19 19\n18 16\n26 25\n6 6\n5 5\n6 4\n5 5\n10 9\n8 7\n9 9\n8 8\n6 6\n10 9\n8 8\n28 17\n4 4\n11 11\n4 3\n24 22\n8 7\n20 19\n5 4\n11 9\n11 10\n12 12\n25 23\n24 24\n14 14\n18 18\n18 18\n17 17\n21 21\n6 4\n17 11\n9 8\n4 4\n9 9\n6 6\n17 17\n33 33\n9 9\n14 10\n19 16\n5 4\n8 7\n18 15\n23 23\n22 19\n9 7\n24 21\n17 16\n11 8\n5 5\n22 19\n2 2\n4 4\n13 13\n25 23\n8 8\n4 4\n6 5\n15 13\n14 14\n3 2\n10 10\n16 12\n10 10\n4 4\n22 21\n29 29\n9 9\n18 18\n5 5\n8 8\n9 9\n8 7\n17 15\n16 13\n6 5\n6 5\n12 12\n8 7\n6 6\n8 7\n21 21\n8 8\n22 20\n7 6\n12 11\n5 4\n6 5\n4 2\n6 4\n8 7\n22 20\n18 16\n6 6\n34 33\n11 11\n15 15\n4 3\n15 13\n19 19\n4 4\n6 6\n22 13\n10 10\n1 1\n5 5\n8 7\n11 9\n23 21\n42 38\n6 5\n5 4\n9 8\n16 15\n22 22\n6 6\n15 15\n23 22\n18 18\n5 5\n4 2\n5 3\n4 4\n6 5\n30 30\n19 18\n60 60\n10 9\n10 10\n26 24\n19 19\n25 20\n26 26\n23 21\n30 28\n5 4\n13 13\n4 4\n5 4\n7 7\n13 12\n9 6\n29 20\n10 10\n16 14\n23 20\n22 18\n9 9\n15 15\n13 12\n20 19\n26 26\n6 6\n13 11\n14 13\n13 12\n30 29\n15 15\n22 20\n23 21\n6 6\n25 24\n9 9\n28 27\n3 3\n27 27\n27 25\n9 8\n14 14\n29 29\n7 5\n18 14\n6 6\n13 13\n27 25\n22 22\n0 0\n19 14\n21 21\n6 5\n10 8\n6 4\n30 28\n4 4\n17 11\n6 6\n9 9\n13 13\n24 21\n25 25\n6 5\n20 18\n24 24\n30 26\n26 25\n7 7\n5 5\n11 11\n20 20\n17 16\n7 5\n25 25\n23 23\n10 10\n6 5\n11 8\n4 4\n30 28\n22 22\n15 15\n14 13\n17 16\n16 15\n23 22\n24 21\n27 24\n5 4\n26 23\n5 5\n3 1\n22 20\n12 12\n14 14\n8 7\n21 18\n17 16\n9 9\n24 23\n8 8\n33 31\n3 3\n20 19\n10 8\n25 25\n29 28\n18 16\n24 21\n8 5\n31 31\n5 5\n29 26\n5 5\n11 11\n22 19\n29 29\n27 26\n7 5\n28 27\n8 8\n16 15\n5 5\n9 9\n14 14\n4 2\n4 3\n17 16\n23 23\n8 8\n18 18\n7 7\n3 3\n5 4\n24 23\n21 20\n15 15\n11 10\n21 20\n26 23\n5 5\n11 11\n16 15\n13 12\n21 19\n8 7\n9 8\n9 8\n12 8\n6 4\n23 22\n24 19\n3 3\n3 3\n19 19\n14 14\n8 3\n5 4\n23 21\n5 5\n7 7\n10 10\n4 4\n8 7\n15 15\n16 14\n7 6\n5 5\n4 4\n9 9\n30 27\n5 4\n4 1\n8 8\n9 7\n6 2\n24 20\n19 17\n4 3\n6 6\n5 4\n15 14\n13 13\n11 6\n15 15\n10 9\n6 6\n8 7\n27 25\n15 15\n13 12\n5 4\n2 2\n4 4\n24 23\n16 13\n27 21\n2 2\n11 10\n14 14\n4 3\n12 12\n9 9\n22 22\n8 8\n5 5\n7 5\n12 12\n5 5\n13 13\n6 6\n17 16\n11 9\n8 6\n4 4\n8 8\n8 7\n30 30\n20 17\n18 15\n14 12\n22 22\n8 8\n7 6\n16 16\n2 2\n12 11\n9 9\n18 17\n4 4\n11 9\n4 4\n21 20\n5 5\n5 5\n8 7\n5 5\n16 16\n8 8\n1 1\n10 9\n12 11\n5 5\n23 23\n15 15\n16 16\n15 11\n26 22\n5 5\n16 15\n14 13\n6 5\n4 3\n9 7\n6 6\n5 4\n20 16\n16 16\n22 20\n16 14\n9 8\n9 9\n11 11\n3 1\n10 8\n28 24\n25 25\n70 66\n12 12\n17 16\n49 43\n8 6\n2 2\n21 20\n27 26\n8 8\n7 6\n25 22\n6 6\n23 18\n10 7\n19 19\n18 18\n8 8\n9 9\n16 12\n8 7\n13 12\n46 40\n9 9\n9 9\n3 2\n12 10\n13 12\n6 4\n9 9\n31 27\n4 4\n15 12\n5 5\n6 4\n30 28\n22 21\n5 5\n9 8\n12 11\n31 30\n4 2\n5 5\n22 20\n5 4\n23 21\n6 6\n36 32\n16 15\n24 23\n6 5\n27 26\n12 11\n6 6\n7 5\n11 11\n4 4\n7 5\n8 8\n5 5\n13 12\n8 8\n26 19\n6 3\n2 2\n7 5\n20 20\n3 2\n25 25\n6 6\n5 5\n18 17\n7 6\n4 4\n7 7\n1 1\n23 21\n6 5\n20 20\n29 23\n17 14\n11 7\n23 23\n14 13\n5 5\n8 7\n7 5\n7 6\n22 17\n6 4\n6 5\n8 7\n10 9\n7 6\n28 27\n13 13\n8 8\n6 6\n16 14\n13 7\n26 24\n15 15\n22 20\n7 7\n17 14\n10 10\n7 7\n5 5\n12 11\n16 16\n27 27\n18 16\n11 11\n4 1\n9 9\n6 6\n6 6\n21 20\n13 13\n1 1\n27 26\n9 9\n26 25\n6 6\n20 18\n3 3\n22 18\n9 8\n6 6\n7 6\n5 3\n16 12\n4 4\n7 7\n25 23\n4 4\n7 6\n31 31\n6 5\n8 8\n10 9\n12 12\n23 21\n14 13\n6 5\n11 11\n12 11\n23 22\n14 14\n4 3\n1 1\n6 5\n5 5\n16 16\n8 8\n4 4\n26 17\n12 12\n12 11\n20 17\n5 5\n7 7\n10 10\n15 15\n23 22\n7 6\n6 6\n6 4\n12 11\n10 8\n13 13\n12 12\n4 4\n22 21\n6 6\n15 14\n14 14\n8 5\n7 6\n30 30\n6 6\n17 16\n25 22\n6 6\n25 25\n24 18\n19 17\n7 7\n14 13\n16 15\n13 11\n6 6\n4 4\n17 15\n23 21\n12 12\n4 4\n9 9\n22 17\n1 1\n5 4\n30 27\n5 5\n16 15\n19 19\n28 27\n18 16\n21 18\n9 7\n15 15\n14 13\n17 17\n5 5\n16 16\n5 5\n11 11\n7 7\n19 17\n5 5\n5 5\n19 19\n5 5\n5 5\n29 27\n16 16\n1 1\n2 2\n11 11\n7 5\n22 20\n12 9\n26 25\n10 10\n5 5\n15 12\n24 23\n17 17\n7 5\n42 39\n23 23\n22 17\n9 8\n14 13\n15 14\n18 18\n1 1\n14 14\n68 50\n8 8\n58 51\n9 7\n13 12\n17 14\n25 18\n16 10\n18 18\n21 20\n4 3\n8 8\n30 27\n16 14\n8 6\n2 2\n27 25\n5 5\n25 25\n7 5\n11 11\n4 4\n7 5\n7 6\n9 9\n12 11\n20 19\n15 15\n19 18\n15 15\n8 7\n8 8\n6 5\n6 5\n9 6\n26 26\n9 9\n7 7\n6 6\n25 24\n8 5\n15 15\n13 3\n6 6\n8 7\n42 38\n24 19\n7 5\n15 14\n15 14\n25 23\n24 19\n5 3\n8 8\n10 10\n30 27\n5 5\n15 14\n6 6\n14 14\n5 4\n19 19\n9 7\n18 18\n13 12\n3 3\n5 3\n10 8\n7 6\n5 4\n7 5\n20 17\n28 20\n7 4\n26 26\n14 12\n21 21\n29 23\n13 11\n24 20\n6 5\n10 10\n29 23\n50 44\n7 5\n11 11\n26 20\n9 7\n14 10\n5 5\n25 23\n4 3\n47 45\n7 7\n25 22\n38 36\n5 4\n5 5\n7 7\n24 22\n24 23\n6 6\n4 4\n18 16\n4 4\n18 18\n5 4\n20 19\n6 6\n12 9\n13 12\n3 3\n15 12\n41 36\n2 2\n5 5\n7 7\n7 6\n10 7\n6 6\n14 13\n23 23\n6 6\n25 24\n7 6\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(w2v_vect):\n",
    "    print(len(X_test.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vect_avg = []\n",
    "\n",
    "#returns word vector averages across each post\n",
    "for vect in w2v_vect:\n",
    "    if len(vect)!= 0:\n",
    "        w2v_vect_avg.append(vect.mean(axis=0))\n",
    "    else:\n",
    "            w2v_vect_avg.append(np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11 100\n",
      "5 100\n",
      "6 100\n",
      "26 100\n",
      "22 100\n",
      "5 100\n",
      "3 100\n",
      "6 100\n",
      "5 100\n",
      "25 100\n",
      "20 100\n",
      "8 100\n",
      "8 100\n",
      "7 100\n",
      "31 100\n",
      "19 100\n",
      "26 100\n",
      "11 100\n",
      "7 100\n",
      "7 100\n",
      "46 100\n",
      "6 100\n",
      "7 100\n",
      "6 100\n",
      "13 100\n",
      "12 100\n",
      "6 100\n",
      "22 100\n",
      "12 100\n",
      "7 100\n",
      "14 100\n",
      "6 100\n",
      "9 100\n",
      "4 100\n",
      "9 100\n",
      "5 100\n",
      "17 100\n",
      "19 100\n",
      "20 100\n",
      "24 100\n",
      "6 100\n",
      "7 100\n",
      "9 100\n",
      "6 100\n",
      "22 100\n",
      "11 100\n",
      "8 100\n",
      "25 100\n",
      "3 100\n",
      "5 100\n",
      "23 100\n",
      "23 100\n",
      "6 100\n",
      "8 100\n",
      "19 100\n",
      "8 100\n",
      "6 100\n",
      "1 100\n",
      "11 100\n",
      "11 100\n",
      "30 100\n",
      "14 100\n",
      "7 100\n",
      "7 100\n",
      "6 100\n",
      "17 100\n",
      "21 100\n",
      "21 100\n",
      "9 100\n",
      "8 100\n",
      "7 100\n",
      "6 100\n",
      "4 100\n",
      "23 100\n",
      "8 100\n",
      "42 100\n",
      "15 100\n",
      "26 100\n",
      "9 100\n",
      "25 100\n",
      "81 100\n",
      "16 100\n",
      "4 100\n",
      "8 100\n",
      "7 100\n",
      "47 100\n",
      "7 100\n",
      "6 100\n",
      "2 100\n",
      "7 100\n",
      "9 100\n",
      "20 100\n",
      "24 100\n",
      "21 100\n",
      "6 100\n",
      "31 100\n",
      "20 100\n",
      "13 100\n",
      "9 100\n",
      "5 100\n",
      "17 100\n",
      "13 100\n",
      "13 100\n",
      "8 100\n",
      "15 100\n",
      "19 100\n",
      "14 100\n",
      "14 100\n",
      "5 100\n",
      "6 100\n",
      "5 100\n",
      "27 100\n",
      "12 100\n",
      "7 100\n",
      "22 100\n",
      "2 100\n",
      "14 100\n",
      "8 100\n",
      "10 100\n",
      "13 100\n",
      "21 100\n",
      "8 100\n",
      "6 100\n",
      "10 100\n",
      "9 100\n",
      "7 100\n",
      "29 100\n",
      "13 100\n",
      "11 100\n",
      "6 100\n",
      "26 100\n",
      "32 100\n",
      "31 100\n",
      "21 100\n",
      "8 100\n",
      "9 100\n",
      "7 100\n",
      "14 100\n",
      "14 100\n",
      "8 100\n",
      "9 100\n",
      "4 100\n",
      "8 100\n",
      "29 100\n",
      "14 100\n",
      "28 100\n",
      "10 100\n",
      "21 100\n",
      "4 100\n",
      "47 100\n",
      "5 100\n",
      "4 100\n",
      "5 100\n",
      "10 100\n",
      "17 100\n",
      "1 100\n",
      "44 100\n",
      "4 100\n",
      "6 100\n",
      "23 100\n",
      "20 100\n",
      "18 100\n",
      "18 100\n",
      "9 100\n",
      "12 100\n",
      "4 100\n",
      "15 100\n",
      "9 100\n",
      "5 100\n",
      "74 100\n",
      "14 100\n",
      "5 100\n",
      "7 100\n",
      "176 100\n",
      "7 100\n",
      "7 100\n",
      "24 100\n",
      "17 100\n",
      "4 100\n",
      "10 100\n",
      "8 100\n",
      "18 100\n",
      "30 100\n",
      "21 100\n",
      "4 100\n",
      "12 100\n",
      "9 100\n",
      "24 100\n",
      "25 100\n",
      "13 100\n",
      "15 100\n",
      "4 100\n",
      "29 100\n",
      "15 100\n",
      "5 100\n",
      "2 100\n",
      "5 100\n",
      "11 100\n",
      "7 100\n",
      "9 100\n",
      "25 100\n",
      "24 100\n",
      "4 100\n",
      "17 100\n",
      "24 100\n",
      "17 100\n",
      "110 100\n",
      "23 100\n",
      "17 100\n",
      "6 100\n",
      "1 100\n",
      "11 100\n",
      "6 100\n",
      "12 100\n",
      "4 100\n",
      "7 100\n",
      "6 100\n",
      "12 100\n",
      "7 100\n",
      "5 100\n",
      "12 100\n",
      "24 100\n",
      "5 100\n",
      "14 100\n",
      "7 100\n",
      "7 100\n",
      "15 100\n",
      "9 100\n",
      "18 100\n",
      "10 100\n",
      "6 100\n",
      "11 100\n",
      "5 100\n",
      "20 100\n",
      "10 100\n",
      "7 100\n",
      "8 100\n",
      "8 100\n",
      "18 100\n",
      "6 100\n",
      "28 100\n",
      "9 100\n",
      "5 100\n",
      "25 100\n",
      "6 100\n",
      "5 100\n",
      "6 100\n",
      "14 100\n",
      "12 100\n",
      "16 100\n",
      "21 100\n",
      "11 100\n",
      "10 100\n",
      "24 100\n",
      "24 100\n",
      "7 100\n",
      "10 100\n",
      "17 100\n",
      "29 100\n",
      "7 100\n",
      "6 100\n",
      "28 100\n",
      "32 100\n",
      "26 100\n",
      "8 100\n",
      "26 100\n",
      "12 100\n",
      "4 100\n",
      "29 100\n",
      "10 100\n",
      "5 100\n",
      "7 100\n",
      "8 100\n",
      "20 100\n",
      "16 100\n",
      "9 100\n",
      "26 100\n",
      "4 100\n",
      "7 100\n",
      "6 100\n",
      "6 100\n",
      "23 100\n",
      "16 100\n",
      "8 100\n",
      "27 100\n",
      "13 100\n",
      "15 100\n",
      "8 100\n",
      "15 100\n",
      "5 100\n",
      "27 100\n",
      "31 100\n",
      "26 100\n",
      "8 100\n",
      "6 100\n",
      "12 100\n",
      "13 100\n",
      "20 100\n",
      "24 100\n",
      "23 100\n",
      "22 100\n",
      "11 100\n",
      "14 100\n",
      "6 100\n",
      "25 100\n",
      "3 100\n",
      "9 100\n",
      "11 100\n",
      "12 100\n",
      "25 100\n",
      "21 100\n",
      "25 100\n",
      "28 100\n",
      "7 100\n",
      "17 100\n",
      "6 100\n",
      "22 100\n",
      "22 100\n",
      "7 100\n",
      "22 100\n",
      "5 100\n",
      "7 100\n",
      "5 100\n",
      "25 100\n",
      "24 100\n",
      "7 100\n",
      "18 100\n",
      "8 100\n",
      "20 100\n",
      "22 100\n",
      "24 100\n",
      "26 100\n",
      "23 100\n",
      "9 100\n",
      "25 100\n",
      "12 100\n",
      "25 100\n",
      "18 100\n",
      "23 100\n",
      "5 100\n",
      "3 100\n",
      "16 100\n",
      "11 100\n",
      "11 100\n",
      "6 100\n",
      "19 100\n",
      "18 100\n",
      "26 100\n",
      "6 100\n",
      "5 100\n",
      "6 100\n",
      "5 100\n",
      "10 100\n",
      "8 100\n",
      "9 100\n",
      "8 100\n",
      "6 100\n",
      "10 100\n",
      "8 100\n",
      "28 100\n",
      "4 100\n",
      "11 100\n",
      "4 100\n",
      "24 100\n",
      "8 100\n",
      "20 100\n",
      "5 100\n",
      "11 100\n",
      "11 100\n",
      "12 100\n",
      "25 100\n",
      "24 100\n",
      "14 100\n",
      "18 100\n",
      "18 100\n",
      "17 100\n",
      "21 100\n",
      "6 100\n",
      "17 100\n",
      "9 100\n",
      "4 100\n",
      "9 100\n",
      "6 100\n",
      "17 100\n",
      "33 100\n",
      "9 100\n",
      "14 100\n",
      "19 100\n",
      "5 100\n",
      "8 100\n",
      "18 100\n",
      "23 100\n",
      "22 100\n",
      "9 100\n",
      "24 100\n",
      "17 100\n",
      "11 100\n",
      "5 100\n",
      "22 100\n",
      "2 100\n",
      "4 100\n",
      "13 100\n",
      "25 100\n",
      "8 100\n",
      "4 100\n",
      "6 100\n",
      "15 100\n",
      "14 100\n",
      "3 100\n",
      "10 100\n",
      "16 100\n",
      "10 100\n",
      "4 100\n",
      "22 100\n",
      "29 100\n",
      "9 100\n",
      "18 100\n",
      "5 100\n",
      "8 100\n",
      "9 100\n",
      "8 100\n",
      "17 100\n",
      "16 100\n",
      "6 100\n",
      "6 100\n",
      "12 100\n",
      "8 100\n",
      "6 100\n",
      "8 100\n",
      "21 100\n",
      "8 100\n",
      "22 100\n",
      "7 100\n",
      "12 100\n",
      "5 100\n",
      "6 100\n",
      "4 100\n",
      "6 100\n",
      "8 100\n",
      "22 100\n",
      "18 100\n",
      "6 100\n",
      "34 100\n",
      "11 100\n",
      "15 100\n",
      "4 100\n",
      "15 100\n",
      "19 100\n",
      "4 100\n",
      "6 100\n",
      "22 100\n",
      "10 100\n",
      "1 100\n",
      "5 100\n",
      "8 100\n",
      "11 100\n",
      "23 100\n",
      "42 100\n",
      "6 100\n",
      "5 100\n",
      "9 100\n",
      "16 100\n",
      "22 100\n",
      "6 100\n",
      "15 100\n",
      "23 100\n",
      "18 100\n",
      "5 100\n",
      "4 100\n",
      "5 100\n",
      "4 100\n",
      "6 100\n",
      "30 100\n",
      "19 100\n",
      "60 100\n",
      "10 100\n",
      "10 100\n",
      "26 100\n",
      "19 100\n",
      "25 100\n",
      "26 100\n",
      "23 100\n",
      "30 100\n",
      "5 100\n",
      "13 100\n",
      "4 100\n",
      "5 100\n",
      "7 100\n",
      "13 100\n",
      "9 100\n",
      "29 100\n",
      "10 100\n",
      "16 100\n",
      "23 100\n",
      "22 100\n",
      "9 100\n",
      "15 100\n",
      "13 100\n",
      "20 100\n",
      "26 100\n",
      "6 100\n",
      "13 100\n",
      "14 100\n",
      "13 100\n",
      "30 100\n",
      "15 100\n",
      "22 100\n",
      "23 100\n",
      "6 100\n",
      "25 100\n",
      "9 100\n",
      "28 100\n",
      "3 100\n",
      "27 100\n",
      "27 100\n",
      "9 100\n",
      "14 100\n",
      "29 100\n",
      "7 100\n",
      "18 100\n",
      "6 100\n",
      "13 100\n",
      "27 100\n",
      "22 100\n",
      "0 100\n",
      "19 100\n",
      "21 100\n",
      "6 100\n",
      "10 100\n",
      "6 100\n",
      "30 100\n",
      "4 100\n",
      "17 100\n",
      "6 100\n",
      "9 100\n",
      "13 100\n",
      "24 100\n",
      "25 100\n",
      "6 100\n",
      "20 100\n",
      "24 100\n",
      "30 100\n",
      "26 100\n",
      "7 100\n",
      "5 100\n",
      "11 100\n",
      "20 100\n",
      "17 100\n",
      "7 100\n",
      "25 100\n",
      "23 100\n",
      "10 100\n",
      "6 100\n",
      "11 100\n",
      "4 100\n",
      "30 100\n",
      "22 100\n",
      "15 100\n",
      "14 100\n",
      "17 100\n",
      "16 100\n",
      "23 100\n",
      "24 100\n",
      "27 100\n",
      "5 100\n",
      "26 100\n",
      "5 100\n",
      "3 100\n",
      "22 100\n",
      "12 100\n",
      "14 100\n",
      "8 100\n",
      "21 100\n",
      "17 100\n",
      "9 100\n",
      "24 100\n",
      "8 100\n",
      "33 100\n",
      "3 100\n",
      "20 100\n",
      "10 100\n",
      "25 100\n",
      "29 100\n",
      "18 100\n",
      "24 100\n",
      "8 100\n",
      "31 100\n",
      "5 100\n",
      "29 100\n",
      "5 100\n",
      "11 100\n",
      "22 100\n",
      "29 100\n",
      "27 100\n",
      "7 100\n",
      "28 100\n",
      "8 100\n",
      "16 100\n",
      "5 100\n",
      "9 100\n",
      "14 100\n",
      "4 100\n",
      "4 100\n",
      "17 100\n",
      "23 100\n",
      "8 100\n",
      "18 100\n",
      "7 100\n",
      "3 100\n",
      "5 100\n",
      "24 100\n",
      "21 100\n",
      "15 100\n",
      "11 100\n",
      "21 100\n",
      "26 100\n",
      "5 100\n",
      "11 100\n",
      "16 100\n",
      "13 100\n",
      "21 100\n",
      "8 100\n",
      "9 100\n",
      "9 100\n",
      "12 100\n",
      "6 100\n",
      "23 100\n",
      "24 100\n",
      "3 100\n",
      "3 100\n",
      "19 100\n",
      "14 100\n",
      "8 100\n",
      "5 100\n",
      "23 100\n",
      "5 100\n",
      "7 100\n",
      "10 100\n",
      "4 100\n",
      "8 100\n",
      "15 100\n",
      "16 100\n",
      "7 100\n",
      "5 100\n",
      "4 100\n",
      "9 100\n",
      "30 100\n",
      "5 100\n",
      "4 100\n",
      "8 100\n",
      "9 100\n",
      "6 100\n",
      "24 100\n",
      "19 100\n",
      "4 100\n",
      "6 100\n",
      "5 100\n",
      "15 100\n",
      "13 100\n",
      "11 100\n",
      "15 100\n",
      "10 100\n",
      "6 100\n",
      "8 100\n",
      "27 100\n",
      "15 100\n",
      "13 100\n",
      "5 100\n",
      "2 100\n",
      "4 100\n",
      "24 100\n",
      "16 100\n",
      "27 100\n",
      "2 100\n",
      "11 100\n",
      "14 100\n",
      "4 100\n",
      "12 100\n",
      "9 100\n",
      "22 100\n",
      "8 100\n",
      "5 100\n",
      "7 100\n",
      "12 100\n",
      "5 100\n",
      "13 100\n",
      "6 100\n",
      "17 100\n",
      "11 100\n",
      "8 100\n",
      "4 100\n",
      "8 100\n",
      "8 100\n",
      "30 100\n",
      "20 100\n",
      "18 100\n",
      "14 100\n",
      "22 100\n",
      "8 100\n",
      "7 100\n",
      "16 100\n",
      "2 100\n",
      "12 100\n",
      "9 100\n",
      "18 100\n",
      "4 100\n",
      "11 100\n",
      "4 100\n",
      "21 100\n",
      "5 100\n",
      "5 100\n",
      "8 100\n",
      "5 100\n",
      "16 100\n",
      "8 100\n",
      "1 100\n",
      "10 100\n",
      "12 100\n",
      "5 100\n",
      "23 100\n",
      "15 100\n",
      "16 100\n",
      "15 100\n",
      "26 100\n",
      "5 100\n",
      "16 100\n",
      "14 100\n",
      "6 100\n",
      "4 100\n",
      "9 100\n",
      "6 100\n",
      "5 100\n",
      "20 100\n",
      "16 100\n",
      "22 100\n",
      "16 100\n",
      "9 100\n",
      "9 100\n",
      "11 100\n",
      "3 100\n",
      "10 100\n",
      "28 100\n",
      "25 100\n",
      "70 100\n",
      "12 100\n",
      "17 100\n",
      "49 100\n",
      "8 100\n",
      "2 100\n",
      "21 100\n",
      "27 100\n",
      "8 100\n",
      "7 100\n",
      "25 100\n",
      "6 100\n",
      "23 100\n",
      "10 100\n",
      "19 100\n",
      "18 100\n",
      "8 100\n",
      "9 100\n",
      "16 100\n",
      "8 100\n",
      "13 100\n",
      "46 100\n",
      "9 100\n",
      "9 100\n",
      "3 100\n",
      "12 100\n",
      "13 100\n",
      "6 100\n",
      "9 100\n",
      "31 100\n",
      "4 100\n",
      "15 100\n",
      "5 100\n",
      "6 100\n",
      "30 100\n",
      "22 100\n",
      "5 100\n",
      "9 100\n",
      "12 100\n",
      "31 100\n",
      "4 100\n",
      "5 100\n",
      "22 100\n",
      "5 100\n",
      "23 100\n",
      "6 100\n",
      "36 100\n",
      "16 100\n",
      "24 100\n",
      "6 100\n",
      "27 100\n",
      "12 100\n",
      "6 100\n",
      "7 100\n",
      "11 100\n",
      "4 100\n",
      "7 100\n",
      "8 100\n",
      "5 100\n",
      "13 100\n",
      "8 100\n",
      "26 100\n",
      "6 100\n",
      "2 100\n",
      "7 100\n",
      "20 100\n",
      "3 100\n",
      "25 100\n",
      "6 100\n",
      "5 100\n",
      "18 100\n",
      "7 100\n",
      "4 100\n",
      "7 100\n",
      "1 100\n",
      "23 100\n",
      "6 100\n",
      "20 100\n",
      "29 100\n",
      "17 100\n",
      "11 100\n",
      "23 100\n",
      "14 100\n",
      "5 100\n",
      "8 100\n",
      "7 100\n",
      "7 100\n",
      "22 100\n",
      "6 100\n",
      "6 100\n",
      "8 100\n",
      "10 100\n",
      "7 100\n",
      "28 100\n",
      "13 100\n",
      "8 100\n",
      "6 100\n",
      "16 100\n",
      "13 100\n",
      "26 100\n",
      "15 100\n",
      "22 100\n",
      "7 100\n",
      "17 100\n",
      "10 100\n",
      "7 100\n",
      "5 100\n",
      "12 100\n",
      "16 100\n",
      "27 100\n",
      "18 100\n",
      "11 100\n",
      "4 100\n",
      "9 100\n",
      "6 100\n",
      "6 100\n",
      "21 100\n",
      "13 100\n",
      "1 100\n",
      "27 100\n",
      "9 100\n",
      "26 100\n",
      "6 100\n",
      "20 100\n",
      "3 100\n",
      "22 100\n",
      "9 100\n",
      "6 100\n",
      "7 100\n",
      "5 100\n",
      "16 100\n",
      "4 100\n",
      "7 100\n",
      "25 100\n",
      "4 100\n",
      "7 100\n",
      "31 100\n",
      "6 100\n",
      "8 100\n",
      "10 100\n",
      "12 100\n",
      "23 100\n",
      "14 100\n",
      "6 100\n",
      "11 100\n",
      "12 100\n",
      "23 100\n",
      "14 100\n",
      "4 100\n",
      "1 100\n",
      "6 100\n",
      "5 100\n",
      "16 100\n",
      "8 100\n",
      "4 100\n",
      "26 100\n",
      "12 100\n",
      "12 100\n",
      "20 100\n",
      "5 100\n",
      "7 100\n",
      "10 100\n",
      "15 100\n",
      "23 100\n",
      "7 100\n",
      "6 100\n",
      "6 100\n",
      "12 100\n",
      "10 100\n",
      "13 100\n",
      "12 100\n",
      "4 100\n",
      "22 100\n",
      "6 100\n",
      "15 100\n",
      "14 100\n",
      "8 100\n",
      "7 100\n",
      "30 100\n",
      "6 100\n",
      "17 100\n",
      "25 100\n",
      "6 100\n",
      "25 100\n",
      "24 100\n",
      "19 100\n",
      "7 100\n",
      "14 100\n",
      "16 100\n",
      "13 100\n",
      "6 100\n",
      "4 100\n",
      "17 100\n",
      "23 100\n",
      "12 100\n",
      "4 100\n",
      "9 100\n",
      "22 100\n",
      "1 100\n",
      "5 100\n",
      "30 100\n",
      "5 100\n",
      "16 100\n",
      "19 100\n",
      "28 100\n",
      "18 100\n",
      "21 100\n",
      "9 100\n",
      "15 100\n",
      "14 100\n",
      "17 100\n",
      "5 100\n",
      "16 100\n",
      "5 100\n",
      "11 100\n",
      "7 100\n",
      "19 100\n",
      "5 100\n",
      "5 100\n",
      "19 100\n",
      "5 100\n",
      "5 100\n",
      "29 100\n",
      "16 100\n",
      "1 100\n",
      "2 100\n",
      "11 100\n",
      "7 100\n",
      "22 100\n",
      "12 100\n",
      "26 100\n",
      "10 100\n",
      "5 100\n",
      "15 100\n",
      "24 100\n",
      "17 100\n",
      "7 100\n",
      "42 100\n",
      "23 100\n",
      "22 100\n",
      "9 100\n",
      "14 100\n",
      "15 100\n",
      "18 100\n",
      "1 100\n",
      "14 100\n",
      "68 100\n",
      "8 100\n",
      "58 100\n",
      "9 100\n",
      "13 100\n",
      "17 100\n",
      "25 100\n",
      "16 100\n",
      "18 100\n",
      "21 100\n",
      "4 100\n",
      "8 100\n",
      "30 100\n",
      "16 100\n",
      "8 100\n",
      "2 100\n",
      "27 100\n",
      "5 100\n",
      "25 100\n",
      "7 100\n",
      "11 100\n",
      "4 100\n",
      "7 100\n",
      "7 100\n",
      "9 100\n",
      "12 100\n",
      "20 100\n",
      "15 100\n",
      "19 100\n",
      "15 100\n",
      "8 100\n",
      "8 100\n",
      "6 100\n",
      "6 100\n",
      "9 100\n",
      "26 100\n",
      "9 100\n",
      "7 100\n",
      "6 100\n",
      "25 100\n",
      "8 100\n",
      "15 100\n",
      "13 100\n",
      "6 100\n",
      "8 100\n",
      "42 100\n",
      "24 100\n",
      "7 100\n",
      "15 100\n",
      "15 100\n",
      "25 100\n",
      "24 100\n",
      "5 100\n",
      "8 100\n",
      "10 100\n",
      "30 100\n",
      "5 100\n",
      "15 100\n",
      "6 100\n",
      "14 100\n",
      "5 100\n",
      "19 100\n",
      "9 100\n",
      "18 100\n",
      "13 100\n",
      "3 100\n",
      "5 100\n",
      "10 100\n",
      "7 100\n",
      "5 100\n",
      "7 100\n",
      "20 100\n",
      "28 100\n",
      "7 100\n",
      "26 100\n",
      "14 100\n",
      "21 100\n",
      "29 100\n",
      "13 100\n",
      "24 100\n",
      "6 100\n",
      "10 100\n",
      "29 100\n",
      "50 100\n",
      "7 100\n",
      "11 100\n",
      "26 100\n",
      "9 100\n",
      "14 100\n",
      "5 100\n",
      "25 100\n",
      "4 100\n",
      "47 100\n",
      "7 100\n",
      "25 100\n",
      "38 100\n",
      "5 100\n",
      "5 100\n",
      "7 100\n",
      "24 100\n",
      "24 100\n",
      "6 100\n",
      "4 100\n",
      "18 100\n",
      "4 100\n",
      "18 100\n",
      "5 100\n",
      "20 100\n",
      "6 100\n",
      "12 100\n",
      "13 100\n",
      "3 100\n",
      "15 100\n",
      "41 100\n",
      "2 100\n",
      "5 100\n",
      "7 100\n",
      "7 100\n",
      "10 100\n",
      "6 100\n",
      "14 100\n",
      "23 100\n",
      "6 100\n",
      "25 100\n",
      "7 100\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(w2v_vect_avg):\n",
    "    print(len(X_test.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('../data/spam.csv', encoding='latin-1')\n",
    "\n",
    "messages = messages.drop(labels = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\n",
    "messages.columns = ['label', 'text']\n",
    "\n",
    "labels = np.where(messages['label'] == 'spam', 1, 0)\n",
    "\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'], labels, test_size = .2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_docs = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TaggedDocument(words=['hi', 'always', 'online', 'on', 'yahoo', 'and', 'would', 'like', 'to', 'chat', 'with', 'you', 'someday'], tags=[0])"
      ]
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "tagged_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = gensim.models.Doc2Vec(tagged_docs, vector_size = 100, window = 5, min_count = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 4.5360858e-03,  8.1595620e-03, -1.0379556e-02,  6.0769417e-03,\n",
       "       -2.4734940e-05,  1.4883233e-02,  5.5764602e-03, -1.5850679e-03,\n",
       "        2.0912103e-02, -5.0269864e-03, -4.6300775e-04, -7.0064887e-03,\n",
       "        1.2206671e-02, -5.0578378e-03,  6.5178964e-03,  1.1895093e-02,\n",
       "        7.5116558e-03,  4.7796901e-04, -6.6709728e-03,  6.8978886e-03,\n",
       "        6.5925354e-03,  6.5963357e-03, -3.3740171e-03,  5.6387263e-04,\n",
       "        1.9247457e-02, -6.5375357e-03,  9.6836721e-04, -6.5170159e-03,\n",
       "       -4.1700960e-03, -2.7327125e-03,  4.6014893e-03, -6.9095870e-03,\n",
       "       -1.0222524e-02, -1.9701723e-02, -1.7338556e-03, -1.2298139e-02,\n",
       "       -3.3436582e-04, -5.2618575e-03, -1.7033113e-02, -7.0912070e-03,\n",
       "        9.1505917e-03,  1.1702876e-02, -6.7535131e-03, -1.3994202e-02,\n",
       "       -6.4389320e-04, -9.1106547e-03,  5.8821873e-03, -4.2518121e-03,\n",
       "       -2.8318751e-03,  9.7785369e-03,  2.0142310e-04,  1.5548123e-02,\n",
       "       -7.7053732e-03,  2.8862960e-03,  7.0729614e-03, -2.0116530e-03,\n",
       "       -5.3785937e-03, -1.0317186e-03, -6.1669541e-03, -6.8406570e-03,\n",
       "       -2.1992037e-03,  4.6735425e-03,  2.1280665e-03,  1.8233050e-02,\n",
       "       -4.6497011e-03, -1.7188162e-02, -2.8685008e-03,  2.5950614e-02,\n",
       "       -4.9691759e-03,  5.4530418e-03, -9.5308973e-03,  3.2884709e-03,\n",
       "       -1.8733906e-02, -4.0022423e-03, -1.4872840e-02,  1.4370083e-02,\n",
       "       -4.7840867e-03,  1.9680241e-02, -5.4447739e-03, -3.0587688e-03,\n",
       "        1.3794278e-03, -9.8070083e-04, -9.1086691e-03,  1.1068723e-02,\n",
       "       -1.6469676e-02, -9.9382801e-03, -5.9219743e-03, -1.7320026e-03,\n",
       "        3.6310577e-03, -7.2117350e-03, -4.9794153e-03,  2.7091645e-03,\n",
       "        1.5432952e-02,  1.6201766e-02, -9.5958244e-03,  9.8731872e-03,\n",
       "       -1.8225373e-03, -1.4391564e-03, -7.4005937e-03, -2.8297273e-03],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "d2v_model.infer_vector(['i', 'am', 'fucking', 'gay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(messages['label'] == 'spam', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already up-to-date: Keras in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from Keras) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from Keras) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from Keras) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from Keras) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from h5py->Keras) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/mishkasistrunk/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already up-to-date: tensorflow in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /home/mishkasistrunk/.local/lib/python3.8/site-packages (from tensorflow) (3.7.4.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.24.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/mishkasistrunk/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[408, 826, 134, 234, 44, 185, 1066, 201]"
      ]
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "X_test_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K \n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_padded = pad_sequences(X_train_seq, 50)\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, 50)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of X train is (4457, 50)\nShape of X test is (1115, 50)\nShape of Y train is (4457,)\nShape of Y test is (1115,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Shape of X train is \" + str(X_train_seq_padded.shape))\n",
    "\n",
    "print('Shape of X test is ' + str(X_test_seq_padded.shape))\n",
    "\n",
    "print('Shape of Y train is ' + str(y_train.shape))\n",
    "\n",
    "print('Shape of Y test is ' + str(y_test.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip)(y_true * y_pred, 0 , 1))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_13\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_13 (Embedding)     (None, None, 32)          219904    \n_________________________________________________________________\nlstm_13 (LSTM)               (None, 32)                8320      \n_________________________________________________________________\ndense_26 (Dense)             (None, 32)                1056      \n_________________________________________________________________\ndense_27 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 229,313\nTrainable params: 229,313\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#Embedding/vector for each text message \n",
    "#what are the dimensions of the input?\n",
    "model.add(Embedding(len(tokenizer.index_word)+1, 32))\n",
    "#Prevents overfitting through regulazation\n",
    "model.add(LSTM(32, dropout = 0, recurrent_dropout = 0))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "loss='binary_crossentropy',\n",
    "metrics = ['accuracy', precision_m, recall_m])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-186-0fa9e4e44e08>:2 recall_m  *\n        true_positives = K.sum(K.round(K.clip)(y_true * y_pred, 0 , 1))\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:2529 round\n        return math_ops.round(x)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:900 round\n        x = ops.convert_to_tensor(x, name=\"x\")\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:264 constant\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:281 _constant_impl\n        tensor_util.make_tensor_proto(\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:551 make_tensor_proto\n        raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\n    TypeError: Failed to convert object of type <class 'function'> to Tensor. Contents: <function clip at 0x7f77397044c0>. Consider casting elements to a supported type.\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-e9a0f65ea4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_seq_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_seq_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-186-0fa9e4e44e08>:2 recall_m  *\n        true_positives = K.sum(K.round(K.clip)(y_true * y_pred, 0 , 1))\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:2529 round\n        return math_ops.round(x)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:900 round\n        x = ops.convert_to_tensor(x, name=\"x\")\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:264 constant\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:281 _constant_impl\n        tensor_util.make_tensor_proto(\n    /home/mishkasistrunk/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:551 make_tensor_proto\n        raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\n    TypeError: Failed to convert object of type <class 'function'> to Tensor. Contents: <function clip at 0x7f77397044c0>. Consider casting elements to a supported type.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_seq_padded, y_train, batch_size=32, epochs=10,validation_data=(X_test_seq_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}